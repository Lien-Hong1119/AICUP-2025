{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 路徑設定及安裝"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from groq import Groq\n",
    "from pathlib import Path\n",
    "import whisperx\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm  # ← 這樣匯入的是函數，而非整個模組\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "base_path = Path(r\"your_path\")\n",
    "\n",
    "Validation_Dataset_Formal_entity= base_path / \"Validation_Dataset_Formal_entity.json\"\n",
    "\n",
    "task1_answer_timestamps = base_path / \"task1_answer_timestamps.json\"\n",
    "task1_answer_timestamps_ZH = base_path / \"task1_answer_timestamps_ZH.json\"\n",
    "submission_task1_answer = base_path / \"submission/task1_answer.txt\"\n",
    "submission_task1_answer_S = base_path / \"submission/task1_answer_S.txt\"\n",
    "submission_task1_answer_L = base_path / \"submission/task1_answer_L.txt\"\n",
    "\n",
    "\n",
    "submission_task2_answer_S = base_path / \"submission/task2_answer_S.txt\"\n",
    "submission_task2_answer_L = base_path / \"submission/task2_answer_L.txt\"\n",
    "\n",
    "submission_task2_answer = base_path / \"submission/task2_answer.txt\"\n",
    "submission_task2_answer_LLM2 = base_path / \"submission/task2_answer_LLM2.txt\"\n",
    "submission_task2_answer_LLM2_reasoning = base_path / \"submission/task2_answer_LLM2_reasoning.txt\"\n",
    "\n",
    "# 清理無效\n",
    "submission_task2_answer_clean_invalid = submission_task2_answer.parent / \"task2_answer_clean_invalid.txt\"\n",
    "\n",
    "# 轉成正確類別\n",
    "submission_task2_answer_corrected = submission_task2_answer.parent / \"task2_answer_corrected.txt\"\n",
    "\n",
    "# 規則一\n",
    "submission_task2_answer_rule1 = submission_task2_answer.parent / \"task2_answer_rule1.txt\"\n",
    "\n",
    "# 規則二\n",
    "submission_task2_answer_rule2 = submission_task2_answer.parent / \"task2_answer_rule2.txt\"\n",
    "\n",
    "# 完全清理乾淨\n",
    "submission_task2_answer_cleaned = submission_task2_answer.parent / \"task2_answer_cleaned.txt\"\n",
    "\n",
    "# 5倍標註量\n",
    "submission_task2_answer_duplicated = submission_task2_answer.parent / \"task2_answer_duplicated.txt\"\n",
    "\n",
    "# 對齊時間戳\n",
    "submission_task2_answer_alignment = submission_task2_answer.parent / \"task2_answer_alignment.txt\"\n",
    "\n",
    "# 排序\n",
    "submission_task2_answer_sort = submission_task2_answer.parent / \"task2_answer_sort.txt\"\n",
    "\n",
    "# 最終輸出結果\n",
    "submission_task2_answer_finally = submission_task2_answer.parent / \"task2_answer_finally.txt\"\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# 類別設定\n",
    "train_phi_category = ['PATIENT', 'DOCTOR', 'USERNAME', 'FAMILYNAME', \"PERSONALNAME\",'PROFESSION',\n",
    "                      'ROOM', 'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET', 'CITY',\n",
    "                      'DISTRICT', 'COUNTY', 'STATE', 'COUNTRY', 'ZIP', 'LOCATION-OTHER',\n",
    "                      'AGE',\n",
    "                      'DATE', 'TIME', 'DURATION', 'SET',\n",
    "                      'PHONE', 'FAX', 'EMAIL', 'URL', 'IPADDRESS',\n",
    "                      'SOCIAL_SECURITY_NUMBER', 'MEDICAL_RECORD_NUMBER', 'HEALTH_PLAN_NUMBER', 'ACCOUNT_NUMBER',\n",
    "                      'LICENSE_NUMBER', 'VEHICLE_ID', 'DEVICE_ID', 'BIOMETRIC_ID', 'ID_NUMBER',\n",
    "                      'OTHER']\n",
    "\n",
    "# 確保資料夾存在\n",
    "# wav_dir.mkdir(parents=True, exist_ok=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## task2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM1_prompt"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "special_note = \"\"\"\n",
    "- Extract exact numeric time expressions as TIME only if they refer to a specific clock time (e.g., \"2:00\", \"two\"). Do not label time spans like \"three hours\" or \"six months\" as TIME—those must be labeled as DURATION.\n",
    "- Always label \"today\", \"yesterday\", \"now\", \"tomorrow\", \"Monday\", \"this week\", and \"June\" as DATE.\n",
    "- Label full expressions like \"Monday morning\" or \"Friday night\" as TIME. Do not split them.\n",
    "- If a number clearly refers to a time point (e.g., \"two\", \"maybe two\"), extract it as TIME even if casual.\n",
    "- Label \"three hours\", \"several weeks\", \"six months\", \"a couple of minutes\", etc. as DURATION. These refer to a time span, not a point in time.\n",
    "- Label \"every morning\", \"twice a week\", \"once a day\" as SET. These refer to recurring patterns and must include recurrence indicators like \"every\" or \"once\".\n",
    "- Do not label vague frequency adverbs like \"frequently\", \"occasionally\", \"sometimes\", or \"regularly\" as SET.\n",
    "- Do not label event-triggered phrases (e.g., \"after three hours\", \"before dinner\", \"when I got home\", \"once I arrived\") as TIME, DURATION, or any category. These are conditionals, not independent time expressions.\n",
    "- Prioritize labeling based on the phrase’s standalone semantic meaning, not on its surrounding context.\n",
    "- Extract all valid TIME, DATE, DURATION, or SET expressions that clearly indicate a specific time, even if embedded in broken or casual sentences.\n",
    "- Time-related phrases must independently and concretely express a time point, span, or recurrence. Do not extract phrases that rely on another action to be meaningful.\n",
    "- TIME expressions must refer to a clock time, time of day, or specific point (e.g., \"3 PM\", \"Friday morning\", \"noon\", \"late at night\").\n",
    "- DURATION expressions must refer to a measurable span (e.g., \"four days\", \"two hours\", \"a long time\") with a time unit.\n",
    "- DATE expressions must refer to a calendar reference (e.g., \"Friday\", \"August 12\", \"yesterday\", \"last year\").\n",
    "- SET expressions must refer to a recurring schedule and include recurrence indicators (e.g., \"every Monday\", \"twice a day\").\n",
    "- AGE expressions must contain a clearly stated number that represents a person's age (e.g., \"65\", \"three\", \"twenty-one\"). The age must be explicit and quantifiable.\n",
    "- FAMILYNAME, PERSONALNAME, DOCTOR, and PATIENT: Only extract full, proper given names (e.g., \"Emily\", \"John\"). Do not extract roles or relational phrases (e.g., \"Ivan's dad\").\n",
    "- If the full name is unknown or missing, do not extract FAMILYNAME or PERSONALNAME under any condition.\n",
    "- Label people based on role:\n",
    "  - DOCTOR: Named individuals providing care or diagnosis.\n",
    "  - PATIENT: Named individuals receiving care or diagnosis.\n",
    "  - FAMILYNAME: Named relatives of the patient.\n",
    "  - PERSONALNAME: Named unrelated individuals not acting as doctor, patient, or family.\n",
    "- If the role is unclear, default to PERSONALNAME.\n",
    "- Do not extract LOCATION-OTHER unless it matches a known list. If uncertain, try CITY, STATE, COUNTY, or DISTRICT. If still unclear, do not label.\n",
    "- Label CITY only if the name clearly refers to a real-world city or town. Ambiguous or fictional locations must not be labeled.\n",
    "- Label each entity occurrence individually. If the same entity appears multiple times in the sentence or paragraph, each occurrence must be labeled separately, even if the category and entity are identical.\n",
    "- For each entity occurrence (token), assign exactly one PHI category.\n",
    "- Do not infer or guess entities. Only extract if explicitly stated and clearly match the defined criteria.\n",
    "- Preserve the exact original casing, spacing, and punctuation of extracted entities.\n",
    "- Output format must be strict: CATEGORY: entity. Do not add explanations, justifications, or notes.\n",
    "- Minor errors are acceptable. Missing valid entities is a serious mistake.\n",
    "- If unsure whether an entity exists, attempt extraction. Output PHI:NULL only if no candidate fits after careful checking.\n",
    "- After extraction, verify that all PHI categories present in the sentence have been labeled. Missing eligible entities is considered a major mistake.\n",
    "- If no valid PHI entity can be extracted, output exactly: PHI:NULL.\n",
    "\"\"\"\n",
    "\n",
    "Rules = \"\"\"\n",
    "- Extract each entity exactly as it appears in the text, preserving the original casing, spaces, and punctuation. No normalization, expansion, or abbreviation is allowed.\n",
    "- For each occurrence (token), assign exactly one PHI category. Do not label the same token with multiple categories.\n",
    "- Each entity must be assigned to only one PHI category based on context. Do not label the same entity with multiple categories.\n",
    "- Extract every occurrence of an entity, even if the category and entity are identical and appear multiple times in the same sentence or paragraph. Do not skip, deduplicate, or merge repeated mentions—each instance must be labeled individually.\n",
    "- If the same entity appears multiple times (even at different positions), each occurrence must be extracted separately. Do not deduplicate.\n",
    "- Exclude titles like \"Dr\" or \"Dr.\" when extracting DOCTOR names; only extract the actual name (e.g., \"James\" from \"Dr. James\").\n",
    "- Do not extract generic words like \"hospital\", \"phone\", or \"address\" unless they have specific identifying information (e.g., \"Bamaga Hospital\", \"911\").\n",
    "- Use only the following PHI categories:\n",
    "    'PATIENT', 'DOCTOR', 'PERSONALNAME', 'FAMILYNAME', 'PROFESSION',\n",
    "    'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET', 'CITY', 'STATE',\n",
    "    'COUNTRY', 'COUNTY', 'ZIP', 'LOCATION-OTHER', 'DISTRICT', 'AGE', 'DATE',\n",
    "    'TIME', 'DURATION', 'SET', 'PHONE', 'MEDICAL_RECORD_NUMBER', 'ID_NUMBER'\n",
    "- Format output strictly as: CATEGORY: entity.\n",
    "- PATIENT: if referring to the patient, including by full name.\n",
    "- DOCTOR: if referring to the doctor, including by full name.\n",
    "- FAMILYNAME: for named family members of the patient (e.g., \"John\", \"Maria\") when their family role is clear (e.g., parent, sibling).\n",
    "- PERSONALNAME: for named unrelated persons or bystanders not identified as patient, doctor, or family.\n",
    "- If the role of a person is unclear, assign PERSONALNAME by default.\n",
    "- If a location contains multiple components (e.g., street, city, state), extract each part under the appropriate category.\n",
    "- CITY: Extract only if the location name clearly refers to a real-world city or town, and is not ambiguous, generic, or a district, state, or country (e.g., \"Chicago\", \"Miami\", \"Hamden\", \"San Antonio\", \"Austin\"). Do not classify as CITY if the entity contains numbers. Use ZIP if address-like.\n",
    "- STATE: First-level administrative regions within a country (e.g., \"Delaware\", \"Montana\", \"Oregon\", \"South Australia\", \"Western Australia\", \"Texas\", \"RI\", \"QLD\").\n",
    "- COUNTRY: Use for names of recognized sovereign nations or independent countries (e.g., \"Madagascar\", \"Denmark\", \"France\", \"Japan\", \"Brazil\", \"Australia\", \"USA\").\n",
    "- COUNTY: Mid-level administrative divisions often below state level (e.g., \"Cheshire\").\n",
    "- DISTRICT: Smaller administrative or geographic areas within cities or counties (e.g., \"Greenwich\").\n",
    "- STREET: Full street names with or without numbers (e.g., \"Main Street\", \"456 Maple Avenue\", \"Oxendon\").\n",
    "- Absolutely no inference, guessing, semantic matching, or partial similarity is allowed. Only exact string matches are valid.\n",
    "- If the location name does not exactly match the above list, do not extract it as LOCATION-OTHER under any condition.\n",
    "- LOCATION-OTHER must match a predefined whitelist. Otherwise, classify as CITY, STATE, COUNTY, or DISTRICT, or discard.\n",
    "- Locations that cannot be confidently categorized as CITY, STATE, COUNTRY, COUNTY, DISTRICT, STREET, or exactly match LOCATION-OTHER must not be extracted at all.\n",
    "- Label a location as CITY only if it is explicitly and clearly indicated as a real-world city or town based on the context. If the location is ambiguous, fictional, incomplete, or contextually unclear, do not label it.\n",
    "- HOSPITAL: Extract the full name of any hospital, medical center, health service, or healthcare institution as HOSPITAL. The name must be specific and identifiable. Generic terms without specific names (e.g., \"hospital\", \"health center\") must not be extracted. Classify as HOSPITAL if entity contains keywords like “Health Service”, “District Health”, “Hospital”, “Clinic”, “Medical Centre”.\n",
    "- ORGANIZATION: Extract the names of institutions, companies, libraries, businesses, or organized groups only if they are explicitly and clearly identifiable as organizations based on the text. Example entries include Subway, Divinity School Library, Career Services, seekers workshop, Cambridge, Starbucks, Michaels, Google, Datsun, and Orizia.\n",
    "- Do not extract any health service, hospital, or medical center names as ORGANIZATION. Such entities must be classified as HOSPITAL if specific naming is present.\n",
    "- ZIP: Treat \"postal code\", \"postcode\", \"zip code\", \"ZIP\", \"ZIP number\", \"mail code\", \"delivery code\", and \"postal ZIP\" as ZIP. Use \"area code\" or \"address code\" only if clearly referring to a mailing address.\n",
    "- AGE: Extract only the numeric age of a person. Convert simple number words (e.g., \"five\") to digits (\"5\"). Exclude words like \"years\" or \"old\". Ignore vague phrases (e.g., \"a couple of years\") and non-numeric terms (e.g., \"young\", \"teenager\").\n",
    "- DURATION: Extract only when the expression clearly refers to a measurable length or span of time with an explicit time unit such as \"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", or \"years\".\n",
    "  - Examples: \"two days\", \"past few weeks\", \"10 minutes\", \"six months\", \"a long time\", \"a couple of hours\"\n",
    "  - Do not extract standalone numbers without time units.\n",
    "  - Do not classify calendar expressions such as \"last year\", \"next week\", \"this Saturday\", or \"last Friday\" as DURATION — these should be classified as DATE.\n",
    "  - Do not extract seasonal or event-based phrases such as \"last summer\" or \"next winter\" — these are also DATE.\n",
    "- TIME: Extract when the expression refers to a specific time of day or clock time.\n",
    "  - Examples: \"3 PM\", \"2:30\", \"morning\", \"afternoon\", \"evening\", \"night\", \"last night\", \"middle of the night\", \"Monday morning\", \"next Friday morning\"\n",
    "  - Do not extract vague expressions such as \"soon\", \"later\", or \"sometime\".\n",
    "  - Always reclassify expressions like \"Friday morning\" or \"next Monday morning\" as TIME, not DATE.\n",
    "- DATE: Extract when the expression refers to a specific calendar point, named date, or time-referenced event.\n",
    "  - Examples: \"now\", \"on Friday\", \"Monday\", \"August 5\", \"May\", \"last year\", \"next week\", \"today\", \"yesterday\", \"tomorrow\", \"Christmas\", \"New Year's Eve\"\n",
    "  - Always classify \"today\", \"yesterday\", and \"tomorrow\" as DATE.\n",
    "  - Days of the week like \"Monday\" and \"Friday\" are DATE unless clearly part of a recurring pattern (e.g., \"every Monday\", which is SET).\n",
    "- SET: Extract only when the expression clearly refers to a repeated time pattern or schedule with explicit recurrence keywords.\n",
    "  - Examples: \"twice a week\", \"every Monday\", \"once a day\", \"three times a week\", \"every night\"\n",
    "  - Do not extract vague frequency terms like \"sometimes\", \"occasionally\", \"frequently\", \"usually\", \"normally\"\n",
    "  - SET expressions must include recurrence indicators like \"every\", \"once a\", \"twice per\", or \"three times each\".\n",
    "  - Do not confuse DURATION (\"two weeks\") with SET (\"every two weeks\").\n",
    "- ROOM: Label room numbers or hospital room identifiers, such as “room 302”, “bed A”, or “A3”.\n",
    "- PROFESSION: Extract explicit job titles and professional roles (e.g., \"babysitter\", \"IT\", \"manager\", \"lawyer\", \"a lawyer\", \"engineer\", \"accountant\").\n",
    "  - Do not classify organization names (e.g., \"Google\", \"Starbucks\") or department names (e.g., \"HR department\", \"Trauma team\") as PROFESSION.\n",
    "- DEPARTMENT: Extract all mentions of departments, teams, units, groups, wards, or divisions explicitly described (e.g., \"HR department\", \"Intensive Care Unit\", \"Immunology Department\", \"Trauma team\").\n",
    "  - The mention must clearly include keywords such as \"department\", \"team\", \"unit\", \"group\", or \"ward\" to qualify.\n",
    "  - Departments related to medical, administrative, academic, or organizational contexts are eligible.\n",
    "  - Do not extract general locations, organization names, or professions as DEPARTMENT unless they explicitly include the structural keywords above.\n",
    "- MEDICAL_RECORD_NUMBER: Use for alphanumeric codes that are clearly tied to a patient’s medical record, such as \"1706458.VTX\". Must be supported by context with terms like \"MRN\", \"record number\", or \"medical file\". If the text refers to a value as “medical record number” even without a decimal, still classify as MEDICAL_RECORD_NUMBER.\n",
    "- ID_NUMBER: Use for general-purpose identifiers that are not explicitly linked to medical records. This includes episode numbers, lab numbers, form codes, and internal or personal IDs.\n",
    "  - If the identifier is not clearly labeled as a medical record number (e.g., “MRN” or “medical record number”) and does not contain a decimal point, label it as ID_NUMBER.\n",
    "  - Do not use ID_NUMBER for values that clearly represent date expressions (e.g., \"17th, 2063\", \"17/06/2063\", \"21, 2063\"). Use DATE instead unless context shows it is functioning as an identifier.\n",
    "  - If a value is clearly referred to as an ID number, episode number, or lab number, it may be labeled as ID_NUMBER regardless of format.\n",
    "\n",
    "Other rules:\n",
    "- Do not infer or hallucinate missing entities.\n",
    "- Preserve the original entity casing and spelling.\n",
    "- If no PHI entities are found, output exactly: PHI:NULL.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "fewshot_example = \"\"\"\n",
    "Sentence:\n",
    "Dr. Connie examined patient Florrie Minion at Kangaroo Island Health Service on June 20, 1989. Her medical record number 4402074.WNE and lab ID 44B20748 were recorded in the Department of Cardiology, located at Blue Cow Street, Camden Haven, Western Australia, ZIP 5067.\n",
    "DOCTOR: Connie\n",
    "PATIENT: Florrie Minion\n",
    "HOSPITAL: Kangaroo Island Health Service\n",
    "DATE: June 20, 1989\n",
    "MEDICAL_RECORD_NUMBER: 4402074.WNE\n",
    "ID_NUMBER: 44B20748\n",
    "DEPARTMENT: Department of Cardiology\n",
    "STREET: Blue Cow Street\n",
    "CITY: Camden Haven\n",
    "STATE: Western Australia\n",
    "ZIP: 5067\n",
    "\n",
    "Sentence:\n",
    "Ashley, a chiropractor, visited the HR department twice a week. The chiropractor worked three hours in the evening at the district office in Greenwich, then spoke with Carl and Ivan’s dad from Cheshire County.\n",
    "PERSONALNAME: Ashley\n",
    "PROFESSION: chiropractor\n",
    "DEPARTMENT: HR department\n",
    "SET: twice a week\n",
    "PROFESSION: chiropractor\n",
    "DURATION: three hours\n",
    "TIME: evening\n",
    "DISTRICT: Greenwich\n",
    "PERSONALNAME: Carl\n",
    "FAMILYNAME: Ivan\n",
    "COUNTY: Cheshire\n",
    "\n",
    "Sentence:\n",
    "On Monday morning in Victoria, Western Australia, I met with the organization Orizia. We verified ID 57X22961, confirmed her age is 65, and she stayed previously at P.O. Box 15.\n",
    "TIME: Monday morning\n",
    "STATE: Victoria\n",
    "STATE: Western Australia\n",
    "ORGANIZATION: Orizia\n",
    "ID_NUMBER: 57X22961\n",
    "AGE: 65\n",
    "LOCATION-OTHER: P.O. Box 15\n",
    "\n",
    "Sentence:\n",
    "He came in last year, stayed for three weeks, returned two hours later, then again every Monday morning around two, left yesterday afternoon, and usually came back two hours later once a week for tests. Push him to A3.\n",
    "DATE: last year\n",
    "DURATION: three weeks\n",
    "DURATION: two hours\n",
    "SET: every Monday\n",
    "TIME: Monday morning\n",
    "TIME: two\n",
    "TIME: yesterday afternoon\n",
    "DURATION: two hours\n",
    "SET: once a week\n",
    "ROOM: A3\n",
    "\n",
    "Sentence:\n",
    "Beaconsfield District Health Service confirmed Ramona's follow-up was at 9:11 a.m. on Oxendon Street, Kyabram, ZIP 7000, where her medical record 4402074.WNE was reviewed again. Her lab number was 44B20748.\n",
    "HOSPITAL: Beaconsfield District Health Service\n",
    "PATIENT: Ramona\n",
    "TIME: 9:11 a.m.\n",
    "STREET: Oxendon Street\n",
    "CITY: Kyabram\n",
    "ZIP: 7000\n",
    "MEDICAL_RECORD_NUMBER: 4402074.WNE\n",
    "ID_NUMBER: 44B20748\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 設定"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "Groq_key = \"your_API_key\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=Groq_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 測試 有checkpoint的，LLM1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import random\n",
    "\n",
    "# ======== 包裝：遇到錯誤就停止 ========\n",
    "def chat_with_stop_on_error(messages, fid, model=\"llama3-70b-8192\", max_tokens=512, temperature=0.0):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"[FATAL ERROR] Error at fid {fid}: {error_message}\")\n",
    "        raise RuntimeError(f\"Program stopped due to error at fid {fid}.\")\n",
    "\n",
    "# ======== 啟動時找最後一個 fid 並清理 answer 檔案 ========\n",
    "restart_fid = None\n",
    "if os.path.exists(submission_task2_answer_S):\n",
    "    with open(submission_task2_answer_S, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    if lines:\n",
    "        last_line = lines[-1]\n",
    "        if \"\\t\" in last_line:\n",
    "            restart_fid = last_line.split(\"\\t\")[0]\n",
    "\n",
    "    if restart_fid:\n",
    "        with open(submission_task2_answer_S, encoding=\"utf-8\") as fin:\n",
    "            all_lines = fin.readlines()\n",
    "        with open(submission_task2_answer_S, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for line in all_lines:\n",
    "                if not line.startswith(restart_fid + \"\\t\"):\n",
    "                    fout.write(line)\n",
    "\n",
    "# ======== 核心 prompt 設定 ========\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert at extracting PHI (Protected Health Information) entities from doctor-patient conversations or Daily conversation.\n",
    "\n",
    "Special note:\n",
    "{special_note}\n",
    "Rules:\n",
    "{Rules}\n",
    "Few-shot examples:\n",
    "{fewshot_example}\n",
    "\"\"\"\n",
    "\n",
    "# ======== 主程式：逐句處理，遇錯就停 ========\n",
    "try:\n",
    "    with open(submission_task1_answer_S, encoding=\"utf-8\") as fin, \\\n",
    "         open(submission_task2_answer_S, \"a\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        system_messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        start_processing = restart_fid is None\n",
    "\n",
    "        pbar = tqdm(fin, desc=\"Extracting PHI entities\")\n",
    "        for line in pbar:\n",
    "            if \"\\t\" not in line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                fid, sentence = line.split(\"\\t\", 1)\n",
    "            except ValueError:\n",
    "                print(f\"[SKIP] Line skipped due to unpacking error: {line}\")\n",
    "                continue\n",
    "\n",
    "            pbar.set_description(f\"Extracting PHI entities (FID {fid})\")\n",
    "            if not start_processing:\n",
    "                if fid == restart_fid:\n",
    "                    start_processing = True\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            user_message = [{\"role\": \"user\", \"content\": f\"Sentence:\\n{sentence}\"}]\n",
    "            messages = system_messages + user_message\n",
    "\n",
    "            prediction = chat_with_stop_on_error(messages, fid=fid)\n",
    "            written_preds = []\n",
    "\n",
    "            for pred_line in prediction.splitlines():\n",
    "                if pred_line.upper() == \"PHI:NULL\":\n",
    "                    continue\n",
    "                if \":\" in pred_line:\n",
    "                    category, entity = pred_line.split(\":\", 1)\n",
    "                    fout.write(f\"{fid}\\t{category.strip()}\\t{entity.strip()}\\n\")\n",
    "                    fout.flush()\n",
    "                    written_preds.append(f\"{category.strip()}: {entity.strip()}\")\n",
    "\n",
    "            if written_preds:\n",
    "                print(f\"[WRITE] FID {fid}: {written_preds}\")\n",
    "            else:\n",
    "                print(f\"[WRITE] FID {fid}: (No entities extrated)\")\n",
    "            time.sleep(35 + random.uniform(1, 30))  # 加點 jitter\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"[EXIT] {e}\")\n",
    "    exit(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## S+L"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def load_lines(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "lines_s_2 = load_lines(submission_task2_answer_S)\n",
    "lines_l_2 = load_lines(submission_task2_answer_L)\n",
    "combined_lines_2 = lines_s_2 + lines_l_2\n",
    "\n",
    "def sort_key(line):\n",
    "    parts = line.split(\"\\t\")\n",
    "    try:\n",
    "        return int(parts[0])  # ⬅️ 確保 fid 按照整數比較\n",
    "    except:\n",
    "        return float('inf')  # 排在最後，防止錯行打亂順序\n",
    "\n",
    "\n",
    "\n",
    "combined_sorted_2 = sorted(combined_lines_2, key=sort_key)\n",
    "\n",
    "with open(submission_task2_answer, 'w', encoding='utf-8') as out_file:\n",
    "    for line in combined_sorted_2:\n",
    "        out_file.write(line + '\\n')\n",
    "\n",
    "print(\"Task 2 合併完成（未去重），輸出至:\", submission_task2_answer)\n",
    "\n",
    "lines_s_1 = load_lines(submission_task1_answer_S)\n",
    "lines_l_1 = load_lines(submission_task1_answer_L)\n",
    "combined_lines_1 = lines_s_1 + lines_l_1\n",
    "\n",
    "combined_sorted_1 = sorted(combined_lines_1, key=sort_key)\n",
    "\n",
    "with open(submission_task1_answer, 'w', encoding='utf-8') as out_file:\n",
    "    for line in combined_sorted_1:\n",
    "        out_file.write(line + '\\n')\n",
    "\n",
    "print(\"Task 1 合併完成（未去重），輸出至:\", submission_task1_answer)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 先處理錯誤的標籤/人名全小寫直接去除"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "modified_count = 0\n",
    "removed_count = 0\n",
    "log_entries = []\n",
    "\n",
    "# === 重複詞修剪工具函式（保留順序，只限制次數） ===\n",
    "def limit_repeated_phrases(text, max_repeat=3):\n",
    "    tokens = text.split()\n",
    "    result = []\n",
    "    seen = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        seen[token] += 1\n",
    "        if seen[token] <= max_repeat:\n",
    "            result.append(token)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# === 讀檔與處理 ===\n",
    "with open(submission_task2_answer, \"r\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    if line.count(\"\\t\") < 2:\n",
    "        cleaned_lines.append(line + \"\\n\")\n",
    "        continue  # 格式錯誤跳過\n",
    "\n",
    "    fid, category, entity = line.split(\"\\t\", 2)\n",
    "    original_category = category\n",
    "    original_entity = entity\n",
    "\n",
    "    # === 清理 category 前綴 \"-\" ===\n",
    "    if category.startswith(\"- \"):\n",
    "        category = category[2:].lstrip()\n",
    "    elif category.startswith(\"-\"):\n",
    "        category = category[1:].lstrip()\n",
    "\n",
    "    # === 移除 entity 中的 NULL ===\n",
    "    entity_tokens = entity.split()\n",
    "    entity_tokens = [t for t in entity_tokens if t.upper() != \"NULL\"]\n",
    "    entity = \" \".join(entity_tokens)\n",
    "    if entity != original_entity:\n",
    "        log_entries.append(\n",
    "            f\"[FID {fid}] Removed 'NULL' from entity: \\\"{original_entity}\\\" → \\\"{entity}\\\"\"\n",
    "        )\n",
    "\n",
    "    # === 若 entity 為空，移除整行並記錄 log ===\n",
    "    if not entity.strip():\n",
    "        removed_count += 1\n",
    "        log_entries.append(\n",
    "            f\"[FID {fid}] Removed entry: entity was NULL or became empty after cleaning\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # === 限制 entity 重複詞次數（保留順序） ===\n",
    "    cleaned_entity = limit_repeated_phrases(entity, max_repeat=3)\n",
    "    if cleaned_entity != entity:\n",
    "        log_entries.append(\n",
    "            f\"[FID {fid}] Trimmed repetitive entity: \\\"{entity}\\\" → \\\"{cleaned_entity}\\\"\"\n",
    "        )\n",
    "        entity = cleaned_entity\n",
    "\n",
    "    # === 特殊過濾：ID_NUMBER / MEDICAL_RECORD_NUMBER ===\n",
    "    if category in {\"ID_NUMBER\", \"MEDICAL_RECORD_NUMBER\"}:\n",
    "        if \",\" in entity:\n",
    "            removed_count += 1\n",
    "            log_entries.append(\n",
    "                f\"[FID {fid}] Removed {category} because entity contains a comma: \\\"{entity}\\\"\"\n",
    "            )\n",
    "            continue\n",
    "        if len(entity.strip()) <= 4:\n",
    "            removed_count += 1\n",
    "            log_entries.append(\n",
    "                f\"[FID {fid}] Removed {category} with short entity (≤ 4 chars): \\\"{entity}\\\"\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    # === 移除全小寫的 PERSONALNAME / FAMILYNAME ===\n",
    "    if category in {\"PERSONALNAME\", \"FAMILYNAME\"} and entity.islower():\n",
    "        removed_count += 1\n",
    "        log_entries.append(\n",
    "            f\"[FID {fid}] Removed {category} with all-lowercase entity: \\\"{entity}\\\"\"\n",
    "        )\n",
    "        continue  # 不保留此行\n",
    "\n",
    "    # === 記錄 category 的修正 ===\n",
    "    if category != original_category:\n",
    "        modified_count += 1\n",
    "        log_entries.append(\n",
    "            f\"[FID {fid}] Cleaned CATEGORY: \\\"{original_category}\\\" → \\\"{category}\\\"\"\n",
    "        )\n",
    "\n",
    "    cleaned_lines.append(f\"{fid}\\t{category}\\t{entity}\\n\")\n",
    "\n",
    "# === 寫回原檔案 ===\n",
    "with open(submission_task2_answer, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.writelines(cleaned_lines)\n",
    "\n",
    "# === 輸出 log ===\n",
    "print(f\"[SUMMARY] {modified_count} category values cleaned.\")\n",
    "print(f\"[SUMMARY] {removed_count} entries removed (NULL/empty, short/invalid ID or MRN, all-lowercase name).\\n\")\n",
    "\n",
    "for entry in log_entries:\n",
    "    print(entry)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 測試，第二個LLM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VALID_PHI_CATEGORIES = {\n",
    "    'PROFESSION',\n",
    "    'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET', 'CITY', 'STATE',\n",
    "    'COUNTRY', 'COUNTY', 'ZIP', 'LOCATION-OTHER', 'DISTRICT', 'AGE', 'DATE',\n",
    "    'TIME', 'DURATION', 'SET', 'PHONE', 'MEDICAL_RECORD_NUMBER', 'ID_NUMBER'\n",
    "}\n",
    "\n",
    "LLM2_Rule = f\"\"\"\n",
    "Valid PHI categories (only choose from these):\n",
    "{VALID_PHI_CATEGORIES}\n",
    "\n",
    "Category definitions:\n",
    "- DATE: Refers to a full day or longer period (e.g., today, tomorrow, this week, August 21)\n",
    "- TIME: Refers to a time within a day (e.g., night, morning, 12:50)\n",
    "- DURATION: A period of time (e.g., 15 minutes, several months)\n",
    "- SET: Repeating or scheduled time expressions (e.g., every day, once a week)\n",
    "- ORGANIZATION: Corporate or institutional names (e.g., Sealed Air Corporation)\n",
    "- HOSPITAL: Medical facilities (e.g., Eidsvold Multipurpose Health Service)\n",
    "- DEPARTMENT: Medical or functional departments (e.g., Surgical Unit, Pharmacy)\n",
    "- CITY / STATE / COUNTRY / ZIP / DISTRICT / STREET: As expected\n",
    "\n",
    "Keyword-based classification rules:\n",
    "- DURATION: Time spans like \"ages\", \"15 minutes\", \"3 hours\", \"several days\", \"half an hour\".\n",
    "- SET: Recurring phrases like \"every day\", \"twice a week\", \"one day a week\", \"weekly\", \"monthly\".\n",
    "- DATE: Calendar terms like \"yesterday\", \"Monday\", \"Sunday\", \"May\", \"Easter\", \"today\", \"tomorrow\", \"this weekend\", \"September 12, 2062\", weekdays, months, or \"last week\".\n",
    "- TIME: less day like \"last night\", \"morning\", \"afternoon\", \"tonight\", or clock times (e.g., \"12:50\", \"2:30PM\").\n",
    "\n",
    "Location-based classification rules:\n",
    "- DEPARTMENT: Phrases with \"department\", \"unit\", \"ward\", \"division\", \"section\", \"center\", \"clinic\", \"rooms\", \"central\".\n",
    "- HOSPITAL: Phrases with \"hospital\", \"medical center\", \"health\", \"healthy\", \"health center\", \"service\", \"services\", \"centre\", \"healthcare\".\n",
    "- ORGANIZATION: Names with \"Inc.\", \"Corp.\", \"Ltd.\", \"Group\", \"Corporation\", or agencies.\n",
    "\n",
    "- CITY: Ends with \"City\" or matches a known city name, (e.g., Chicago, Bowen).\n",
    "- STATE: Subnational administrative divisions such as states, territories, or regions (e.g., TAS, NT, Western Australia, California).\n",
    "- COUNTRY: Known country names, (e.g., United States, Germany, India).\n",
    "- ZIP: numeric postal codes (e.g., \"90210\" or \"8003\").\n",
    "- DISTRICT: Contains \"District\", or sub-city regions, (e.g., Greenwich)\n",
    "- COUNTY: Contains \"County\" (e.g., \"Cheshire\").\n",
    "- STREET: Ends with \"St.\", \"Ave\", \"Road\", \"Boulevard\", \"Lane\", etc, (e.g., Legend Manor Street).\n",
    "\n",
    "- PROFESSION: If the entity is a specific job title such as \"lawyer\", \"chiropractor\", \"manager\" or \"cashier\", classify it as PROFESSION.\n",
    "\n",
    "Examples:\n",
    "TIME: this morning\n",
    "<think>\"this morning\" is a time span within the day, so it should be TIME.</think>\n",
    "\n",
    "DATE: last night\n",
    "<think>\"last night\" refers to a night, which is within a day. It should be TIME.</think>\n",
    "\n",
    "CITY: Eidsvold Multipurpose Health Service\n",
    "<think>This is not a city, but a hospital. Correct category is HOSPITAL.</think>\n",
    "\n",
    "HOSPITAL: Timbun and District Healthcare Services\n",
    "<think>\"Timbun and District Healthcare Services\" contains the keywords 'healthcare' and 'services', which match the HOSPITAL category, not ORGANIZATION.</think>\n",
    "\n",
    "DEPARTMENT: 3HR State Surgical Unit\n",
    "<think>\"3HR State Surgical Unit\" contains the keyword 'unit', which is used to describe a department within a hospital, not the hospital itself.</think>\n",
    "\n",
    "DATE: now\n",
    "<think>\"now\" is like 'today'—it refers to the current date, not time of day.</think>\n",
    "\n",
    "DEPARTMENT: clinic\n",
    "<think>\"clinic\" refers to a specific unit or sub-division within a healthcare facility, not a hospital as a whole. It should be categorized as DEPARTMENT, not HOSPITAL.</think>\n",
    "\n",
    "ORGANIZATION: YMCA\n",
    "<think>\"YMCA\" is a non-profit organization, not a hospital or department. It should be classified as ORGANIZATION.</think>\n",
    "\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# ======== 包裝：遇到錯誤就停止 ========\n",
    "def chat_with_stop_on_error_LLM2(messages, fid, model=\"llama3-70b-8192\", max_tokens=512, temperature=0.0):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        if result is None:\n",
    "            print(f\"[FATAL] Empty response at fid {fid} → result is None\")\n",
    "            exit(1)\n",
    "        return result.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e).lower()\n",
    "\n",
    "        if \"429\" in error_message or \"rate limit\" in error_message or \"too many requests\" in error_message:\n",
    "            print(f\"[FATAL] 429 Rate Limit Error at fid {fid} → {error_message}\")\n",
    "            exit(1)\n",
    "\n",
    "        if \"maximum context length\" in error_message or \"maximum tokens\" in error_message or \"token limit\" in error_message:\n",
    "            print(f\"[FATAL] Token Limit Error at fid {fid} → {error_message}\")\n",
    "            exit(1)\n",
    "\n",
    "        print(f\"[FATAL] Unexpected Error at fid {fid} → {error_message}\")\n",
    "        exit(1)\n",
    "\n",
    "# 實作呼叫\n",
    "def process_batch(buffer, fout):\n",
    "    numbered = [f\"{i+1}. {cat}: {ent}\" for i, (_, cat, ent) in enumerate(buffer)]\n",
    "    user_prompt = f\"There are {len(buffer)} lines. Return {len(buffer)} corrected lines:\\n\\n\" + \"\\n\".join(numbered)\n",
    "    messages = system_messages_LLM2 + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    result = chat_with_stop_on_error_LLM2(messages, fid=buffer[0][0])\n",
    "    raw_lines = result.strip().splitlines()\n",
    "\n",
    "    parsed_lines = []\n",
    "    for i, line in enumerate(raw_lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        line = re.sub(r\"^\\d+\\.\\s*\", \"\", line)\n",
    "        parsed_lines.append((i, line))\n",
    "\n",
    "    corrected_lines = []\n",
    "    for i, line in parsed_lines:\n",
    "        if \":\" in line and not line.startswith(\"<\") and not line.lower().startswith(\"here are\"):\n",
    "            try:\n",
    "                category, entity = map(str.strip, line.split(\":\", 1))\n",
    "                corrected_lines.append((i, category, entity))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    corrected_pool = corrected_lines.copy()\n",
    "\n",
    "    for i in range(len(buffer)):\n",
    "        fid, category_orig, entity_orig = buffer[i]\n",
    "        matched = False\n",
    "\n",
    "        # 只相信他的Category，不相信他的entity，因為有時候會汙染，所以說寫進去只寫自己原本的entity\n",
    "        for j, (idx, category_pred, entity_pred) in enumerate(corrected_pool):\n",
    "            # category必須要在合法名單中\n",
    "            if entity_pred == entity_orig and category_pred in VALID_PHI_CATEGORIES:\n",
    "                fout.write(f\"{fid}\\t{category_pred}\\t{entity_orig}\\n\")\n",
    "                fout.flush()\n",
    "                print(f\"[WRITE] {fid}: {category_pred}: {entity_orig}\")\n",
    "\n",
    "                if category_pred != category_orig:\n",
    "                    with open(submission_task2_answer_LLM2_reasoning, \"a\", encoding=\"utf-8\") as fr:\n",
    "                        fr.write(f\"[REPLACED] {fid}\\t{category_orig} → {category_pred}\\t{entity_orig}\\n\")\n",
    "\n",
    "                corrected_pool.pop(j)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            fout.write(f\"{fid}\\t{category_orig}\\t{entity_orig}\\n\")\n",
    "            fout.flush()\n",
    "            print(f\"[FALLBACK] {fid}: {category_orig}: {entity_orig}\")\n",
    "            with open(submission_task2_answer_LLM2_reasoning, \"a\", encoding=\"utf-8\") as fr:\n",
    "                fr.write(f\"[MISMATCH] {fid}\\t{category_orig}\\t{entity_orig} ← LLM2 returned unmatched category or entity\\n\")\n",
    "\n",
    "    buffer.clear()\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# ======== 啟動時找最後一個 fid 並清理 answer 檔案 ========\n",
    "restart_fid_LLM2 = None\n",
    "if os.path.exists(submission_task2_answer_LLM2):\n",
    "    with open(submission_task2_answer_LLM2, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    if lines:\n",
    "        last_line = lines[-1]\n",
    "        if \"\\t\" in last_line:\n",
    "            restart_fid_LLM2 = last_line.split(\"\\t\")[0]\n",
    "\n",
    "    if restart_fid_LLM2:\n",
    "        with open(submission_task2_answer_LLM2, encoding=\"utf-8\") as fin:\n",
    "            all_lines = fin.readlines()\n",
    "        with open(submission_task2_answer_LLM2, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for line in all_lines:\n",
    "                if not line.startswith(restart_fid_LLM2 + \"\\t\"):\n",
    "                    fout.write(line)\n",
    "\n",
    "# ======== 核心 prompt 設定 ========\n",
    "batch_size = 10\n",
    "\n",
    "system_prompt_LLM2 = f\"\"\"\n",
    "You are a PHI category corrector.\n",
    "\n",
    "Each input is in the format: CATEGORY: ENTITY\n",
    "Correct only the CATEGORY if it's in this list:\n",
    "DATE, TIME, DURATION, SET, ORGANIZATION, HOSPITAL, DEPARTMENT, CITY, STATE, COUNTRY, DISTRICT, ZIP, STREET\n",
    "\n",
    "Return exactly one corrected line per input, in the same order.\n",
    "Do not change the ENTITY.\n",
    "Do not add explanations, summaries, or introductions.\n",
    "Output only the corrected lines. No extra text.\n",
    "\n",
    "Strict Output Constraints:\n",
    "- You must return exactly one line per input, no more, no less.\n",
    "- DO NOT merge, summarize, omit, or skip any input line.\n",
    "- Even if two lines appear similar, treat them as independent and return both.\n",
    "- Output must preserve the input order exactly.\n",
    "- If unsure, copy the CATEGORY unchanged.\n",
    "- Do not omit similar-looking inputs. Return every line, even if redundant.\n",
    "- Each input must be treated independently, and classification must strictly follow the Rules.\n",
    "- You are strictly prohibited from using your own judgment to classify entities that appear in the Rules section. If an ENTITY matches any keyword or phrase listed in the Rules, you must assign the exact category as specified. Do not rely on context. Do not reinterpret. Do not override. These mappings are absolute, authoritative, and must be applied exactly as written.\n",
    "- Ignore general knowledge or real-world semantics if they contradict these rules.\n",
    "\n",
    "Output format:\n",
    "CATEGORY: ENTITY\n",
    "\n",
    "Rules:\n",
    "{LLM2_Rule}\n",
    "\"\"\"\n",
    "\n",
    "# 只需要 LLM 處理的類別\n",
    "target_categories = {\n",
    "    'DATE', 'TIME', 'DURATION', 'SET',\n",
    "    'ORGANIZATION', 'HOSPITAL', 'DEPARTMENT',\n",
    "    'CITY', 'STATE', 'COUNTRY', 'DISTRICT', 'ZIP', 'LOCATION-OTHER', 'STREET'\n",
    "}\n",
    "\n",
    "buffer = []\n",
    "# ======== 主程式 ========\n",
    "try:\n",
    "    with open(submission_task2_answer, encoding=\"utf-8\") as llm1_file, \\\n",
    "         open(submission_task2_answer_LLM2, \"a\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        system_messages_LLM2 = [{\"role\": \"system\", \"content\": system_prompt_LLM2}]\n",
    "        start_processing_LLM2 = restart_fid_LLM2 is None\n",
    "\n",
    "        for line in tqdm(llm1_file, desc=\"Processing in batches\"):\n",
    "            if \"\\t\" not in line:\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            fid, category, entity = parts[0], parts[1], parts[2]\n",
    "\n",
    "            if not start_processing_LLM2:\n",
    "                if fid == restart_fid_LLM2:\n",
    "                    start_processing_LLM2 = True\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if category not in target_categories:\n",
    "                fout.write(f\"{fid}\\t{category}\\t{entity}\\n\")\n",
    "                fout.flush()\n",
    "                continue\n",
    "\n",
    "            buffer.append((fid, category, entity))\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                process_batch(buffer, fout)\n",
    "\n",
    "        # 最後不足一批的也要處理\n",
    "        if buffer:\n",
    "            process_batch(buffer, fout)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[EXIT] {e}\")\n",
    "    exit(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 後處理"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 清理submission_task2_answer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 先清理無效的列"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 類別標準化對照表\n",
    "category_mapping = {\n",
    "    # 地址相關\n",
    "    \"address\": \"STREET\",\n",
    "    \"street\": \"STREET\",\n",
    "    \"street name\": \"STREET\",\n",
    "    \"road\": \"STREET\",\n",
    "    \"road name\": \"STREET\",\n",
    "    \"city\": \"CITY\",\n",
    "    \"state\": \"STATE\",\n",
    "    \"zip\": \"ZIP\",\n",
    "    \"zip code\": \"ZIP\",\n",
    "    \"postal code\": \"ZIP\",\n",
    "    \"country\": \"COUNTRY\",\n",
    "    \"county\": \"COUNTY\",\n",
    "    \"district\": \"DISTRICT\",\n",
    "    \"location\": \"LOCATION-OTHER\",\n",
    "    \"location-other\": \"LOCATION-OTHER\",\n",
    "\n",
    "    # 人物相關\n",
    "    \"name\": \"PERSONALNAME\",\n",
    "    \"person name\": \"PERSONALNAME\",\n",
    "    \"personal name\": \"PERSONALNAME\",\n",
    "    \"family name\": \"FAMILYNAME\",\n",
    "    \"patient\": \"PATIENT\",\n",
    "    \"doctor\": \"DOCTOR\",\n",
    "    \"username\": \"USERNAME\",\n",
    "    \"profession\": \"PROFESSION\",\n",
    "\n",
    "    # 聯絡方式\n",
    "    \"phone\": \"PHONE\",\n",
    "    \"phone number\": \"PHONE\",\n",
    "    \"telephone\": \"PHONE\",\n",
    "    \"fax\": \"FAX\",\n",
    "    \"email\": \"EMAIL\",\n",
    "    \"email address\": \"EMAIL\",\n",
    "    \"url\": \"URL\",\n",
    "    \"ip\": \"IPADDRESS\",\n",
    "    \"ip address\": \"IPADDRESS\",\n",
    "\n",
    "    # 醫療與身份編號\n",
    "    \"ssn\": \"SOCIAL_SECURITY_NUMBER\",\n",
    "    \"social security number\": \"SOCIAL_SECURITY_NUMBER\",\n",
    "    \"medical record number\": \"MEDICAL_RECORD_NUMBER\",\n",
    "    \"health plan number\": \"HEALTH_PLAN_NUMBER\",\n",
    "    \"account number\": \"ACCOUNT_NUMBER\",\n",
    "    \"license number\": \"LICENSE_NUMBER\",\n",
    "    \"id\": \"ID_NUMBER\",\n",
    "    \"id number\": \"ID_NUMBER\",\n",
    "    \"lab_number\": \"ID_NUMBER\",\n",
    "    \"device id\": \"DEVICE_ID\",\n",
    "    \"vehicle id\": \"VEHICLE_ID\",\n",
    "    \"biometric id\": \"BIOMETRIC_ID\",\n",
    "\n",
    "    # 其他類型\n",
    "    \"date of birth\": \"DATE\",\n",
    "    \"birthdate\": \"DATE\",\n",
    "    \"dob\": \"DATE\",\n",
    "    \"date\": \"DATE\",\n",
    "    \"age\": \"AGE\",\n",
    "    \"time\": \"TIME\",\n",
    "    \"duration\": \"DURATION\",\n",
    "    \"set\": \"SET\",\n",
    "    \"room\": \"ROOM\",\n",
    "    \"department\": \"DEPARTMENT\",\n",
    "    \"hospital\": \"HOSPITAL\",\n",
    "    \"organization\": \"ORGANIZATION\",\n",
    "    \"other\": \"OTHER\",\n",
    "}\n",
    "\n",
    "# 錯誤行記錄\n",
    "invalid_entries = []\n",
    "\n",
    "with open(submission_task2_answer_LLM2, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_clean_invalid, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or line.upper() == \"PHI:NULL\":\n",
    "            continue\n",
    "\n",
    "        if \"\\t\" not in line:\n",
    "            invalid_entries.append((\"InvalidFormat\", line))\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            invalid_entries.append((\"InvalidParts\", line))\n",
    "            continue\n",
    "\n",
    "        id_, category, value = parts\n",
    "\n",
    "        # 改用 in 判斷\n",
    "        lowered = value.lower()\n",
    "        if \"is not a valid\" in lowered:\n",
    "            parts = lowered.split(\"is not a valid\", 1)\n",
    "            new_value = parts[0].strip()\n",
    "            raw_category = parts[1].strip().split()[0]  # 拿第一個單字\n",
    "            new_category = category_mapping.get(raw_category, raw_category.upper())\n",
    "            fout.write(f\"{id_}\\t{new_category}\\t{new_value}\\n\")\n",
    "             # ✅ 增加這行，把修正資訊也 log 起來\n",
    "            invalid_entries.append({\n",
    "                \"type\": f\"FIXED({new_category})\",\n",
    "                \"original\": value,\n",
    "                \"corrected\": f\"{id_}\\t{new_category}\\t{new_value}\"\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            fout.write(line + \"\\n\")\n",
    "\n",
    "# 錯誤列印\n",
    "if invalid_entries:\n",
    "    print(\"錯誤列：\")\n",
    "    print(f\"總共 {len(invalid_entries)} 行錯誤列\")\n",
    "    for entry in invalid_entries:\n",
    "        if isinstance(entry, dict):  # FIXED 類型\n",
    "            print(f\"[{entry['type']}]\")\n",
    "            print(f\"  原始：{entry['original']}\")\n",
    "            print(f\"  修改：{entry['corrected']}\")\n",
    "        else:  # 其他格式錯誤\n",
    "            category, content = entry\n",
    "            print(f\"[{category}] {content}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## re+去除category == entity相關"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## STREET, CITY, STATE, ZIP"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "tags = ['STREET', 'CITY', 'STATE', 'ZIP']\n",
    "\n",
    "class OptimizedAddressExtractor:\n",
    "    def __init__(self):\n",
    "        # 預編譯所有正則表達式 - 修正版\n",
    "        self.regex_patterns = [\n",
    "            # 標準格式：residing at STREET, CITY, STATE ZIP\n",
    "            re.compile(\n",
    "                r\"residing at\\s+([\\w\\s'.-]{2,40}?),\\s*([\\w\\s'.-]{2,30}?),\\s*([A-Z][a-zA-Z\\s]+|[A-Z]{2,})(?:[,\\s]+(\\d{4}))?\",\n",
    "                re.IGNORECASE\n",
    "            ),\n",
    "            # residing on STREET in CITY, STATE ZIP\n",
    "            re.compile(\n",
    "                r\"residing on\\s+([\\w\\s'.-]{2,40}?)\\s+in\\s+([\\w\\s'.-]{2,30}?),\\s*([A-Z][a-zA-Z\\s]+|[A-Z]{2,})(?:[,\\s]+(\\d{4}))?\",\n",
    "                re.IGNORECASE\n",
    "            ),\n",
    "            # 一般格式：STREET, CITY, STATE ZIP (但要避免句子開頭的長文字)\n",
    "            re.compile(\n",
    "                r\"(?:^|\\.\\s+|\\b(?:at|in)\\s+)([\\w\\s'.-]{2,40}?),\\s*([\\w\\s'.-]{2,30}?),\\s*([A-Z]{2,}|[A-Z][a-zA-Z\\s]+)[,\\s]+(\\d{4})\",\n",
    "                re.IGNORECASE\n",
    "            ),\n",
    "            # residing at STREET in CITY, STATE ZIP\n",
    "            re.compile(\n",
    "                r\"residing at\\s+([\\w\\s'.-]{2,40}?)\\s+in\\s+([\\w\\s'.-]{2,30}?),\\s*([A-Z]{2,}|[A-Z][a-zA-Z\\s]+)(?:[,\\s]+(\\d{4}))?\",\n",
    "                re.IGNORECASE\n",
    "            ),\n",
    "            re.compile(\n",
    "            r\"residing at\\s+([\\w\\s'.-]{2,40}?)\\s+in\\s+([\\w\\s'.-]{2,30}?),\\s*([A-Z]{2,}|[A-Z][a-zA-Z\\s]+)[,\\s]+with the postal code\\s+(\\d{4})\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        ]\n",
    "\n",
    "        # 預編譯驗證相關的正則表達式\n",
    "        self.state_pattern = re.compile(r\"^(?:[A-Z]{2,}|[A-Z][a-z]+(?: [A-Z][a-z]+)*)$\")\n",
    "        self.zip_pattern = re.compile(r\"^\\d{4}$\")\n",
    "\n",
    "        # 加強 STREET 驗證模式\n",
    "        self.street_junk_pattern = re.compile(r\"\\b(resides? at|resides? on|resides? in|located|lives?|residences? in|specimen|collected|department|diagnostic|pathology)\\b\", re.IGNORECASE)\n",
    "        self.street_invalid_pattern = re.compile(r\"^\\d{4}$|^\\d{1,4}\\s*$|Department|Unit|Service|Hospital|Centre|Center|specimen|collected|patient|diagnostic\", re.IGNORECASE)\n",
    "        self.street_valid_pattern = re.compile(r\"^[\\w\\s'.-]{2,40}$\")\n",
    "        self.street_cleanup_pattern = re.compile(r\"\\.\\s*He\\s+resides.*\")\n",
    "\n",
    "        # 年份檢測模式\n",
    "        self.year_pattern = re.compile(r\"^\\d{4}$\")\n",
    "\n",
    "        # 編號/代碼模式檢測（不是有效街道）\n",
    "        self.code_pattern = re.compile(r\"^\\d+[A-Z]+\\d+$|^[A-Z]+\\d+[A-Z]+$|^\\d{2,}[A-Z]{1,3}\\d{2,}$\", re.IGNORECASE)\n",
    "\n",
    "        # 醫療/時間相關詞彙過濾\n",
    "        self.medical_time_pattern = re.compile(r\"\\b(specimen|collected|january|february|march|april|may|june|july|august|september|october|november|december|tissue|pathology|diagnostic|oncology|at\\s+\\d+\\.\\d+)\\b\", re.IGNORECASE)\n",
    "\n",
    "        self.zip_fullmatch_pattern = re.compile(r\"\\d{4}\")\n",
    "\n",
    "        # 預編譯句子預處理的正則表達式\n",
    "        self.preprocessing_patterns = [\n",
    "            (re.compile(r\"\\bresides at\\b\", re.IGNORECASE), \"residing at\"),\n",
    "            (re.compile(r\"\\bresides on\\b\", re.IGNORECASE), \"residing on\"),\n",
    "            (re.compile(r\"\\blives at\\b\", re.IGNORECASE), \"residing at\"),\n",
    "            (re.compile(r\"\\blocated at\\b\", re.IGNORECASE), \"residing at\"),\n",
    "            (re.compile(r\"\\baddress is\\b\", re.IGNORECASE), \"residing at\"),\n",
    "            (re.compile(r\"postal code\", re.IGNORECASE), \"\"),\n",
    "            (re.compile(r\"with postal code\\s*(\\d{4})\", re.IGNORECASE), r\"\\1\"),\n",
    "            (re.compile(r\"ZIP\\s*(\\d{4})\", re.IGNORECASE), r\"\\1\"),\n",
    "            (re.compile(r\"near\\s+([A-Z][a-zA-Z\\s]+|[A-Z]{2,})\", re.IGNORECASE), r\"\\1\")\n",
    "        ]\n",
    "\n",
    "    def is_valid_state(self, text):\n",
    "        \"\"\"驗證州名格式\"\"\"\n",
    "        return bool(self.state_pattern.match(text.strip()))\n",
    "\n",
    "    def is_valid_zip(self, text):\n",
    "        \"\"\"驗證郵遞區號格式\"\"\"\n",
    "        return bool(self.zip_pattern.match(text.strip()))\n",
    "\n",
    "    def preprocess_sentence(self, sentence):\n",
    "        \"\"\"批次預處理句子，提高效率\"\"\"\n",
    "        sentence_norm = sentence\n",
    "        for pattern, replacement in self.preprocessing_patterns:\n",
    "            sentence_norm = pattern.sub(replacement, sentence_norm)\n",
    "        return sentence_norm\n",
    "\n",
    "    def validate_and_clean_street(self, value, fid, log):\n",
    "        \"\"\"驗證和清理街道地址\"\"\"\n",
    "        # 檢查是否為年份\n",
    "        if self.year_pattern.match(value.strip()):\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (年份): {value}\")\n",
    "            return None\n",
    "\n",
    "        # 檢查是否為編號/代碼格式\n",
    "        if self.code_pattern.match(value.strip()):\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (編號/代碼): {value}\")\n",
    "            return None\n",
    "\n",
    "        # 檢查是否包含醫療/時間相關詞彙\n",
    "        if self.medical_time_pattern.search(value):\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (醫療/時間詞彙): {value}\")\n",
    "            return None\n",
    "\n",
    "        # 檢查是否包含無效模式\n",
    "        if self.street_invalid_pattern.search(value):\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (無效模式): {value}\")\n",
    "            return None\n",
    "\n",
    "        # 清理內容\n",
    "        value = self.street_cleanup_pattern.sub(\"\", value).strip()\n",
    "        value = re.sub(r\"^(at|in)\\s+\", \"\", value, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # 檢查垃圾文字 - 在清理後檢查\n",
    "        if self.street_junk_pattern.search(value):\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (包含垃圾詞): {value}\")\n",
    "            return None\n",
    "\n",
    "        # 基本格式驗證\n",
    "        if not self.street_valid_pattern.match(value):\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (格式不符): {value}\")\n",
    "            return None\n",
    "\n",
    "        # 檢查是否太長（可能是整個句子）\n",
    "        if len(value) > 50:\n",
    "            log.append(f\"{fid} - ⚠️ Skip STREET (過長): {value}\")\n",
    "            return None\n",
    "\n",
    "        return value\n",
    "\n",
    "    def categorize_entity(self, tag, value):\n",
    "        \"\"\"僅依 tag 類別統一分類，不細分子類別\"\"\"\n",
    "        return [tag.upper()]\n",
    "\n",
    "\n",
    "    def extract_entities_from_sentence(self, fid, sentence, log):\n",
    "        \"\"\"從單個句子提取地址實體（每條 regex 順序嘗試，驗證失敗繼續）\"\"\"\n",
    "        sentence_norm = self.preprocess_sentence(sentence)\n",
    "\n",
    "        for regex in self.regex_patterns:\n",
    "            match = regex.search(sentence_norm)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            groups = match.groups()\n",
    "            local_entities = []\n",
    "            valid = True\n",
    "\n",
    "            for tag, value in zip(tags, groups):\n",
    "                if not value:\n",
    "                    continue\n",
    "                value = value.strip()\n",
    "\n",
    "                if tag == \"STREET\":\n",
    "                    cleaned_value = self.validate_and_clean_street(value, fid, log)\n",
    "                    if not cleaned_value:\n",
    "                        valid = False\n",
    "                        break\n",
    "                    value = cleaned_value\n",
    "\n",
    "                elif tag == \"STATE\" and not self.is_valid_state(value):\n",
    "                    log.append(f\"{fid} - ❌ Invalid STATE: {value}\")\n",
    "                    valid = False\n",
    "                    break\n",
    "\n",
    "                elif tag == \"ZIP\" and value and not self.is_valid_zip(value):\n",
    "                    log.append(f\"{fid} - ❌ Invalid ZIP: {value}\")\n",
    "                    valid = False\n",
    "                    break\n",
    "\n",
    "                local_entities.append((tag, value))\n",
    "\n",
    "            if valid and len(local_entities) >= 3:\n",
    "                values_only = [val for _, val in local_entities]\n",
    "                log.append(f\"{fid} - ✅ Extracted: {' | '.join(values_only)}\")\n",
    "                return local_entities  # 成功，馬上使用，結束迴圈\n",
    "\n",
    "        return []  # 沒有任何有效匹配\n",
    "\n",
    "    def correct_invalid_tag(self, tag, val, fid, log):\n",
    "        \"\"\"修正無效標籤 - 加強版\"\"\"\n",
    "        tag = tag.upper()\n",
    "        original_tag = tag\n",
    "        corrected_tag = tag\n",
    "\n",
    "        if tag in {\"LOCATION-OTHER\", \"DISTRICT\"}:\n",
    "            val_lower = val.lower()\n",
    "            reason = \"\"\n",
    "\n",
    "            # 檢查是否為醫院/機構名稱（不應該修正為地址）\n",
    "            if re.search(r\"\\b(department|unit|service|hospital|centre|center|clinic|medical)\\b\", val_lower):\n",
    "                corrected_tag = \"DEPARTMENT\"\n",
    "                reason = \"醫療機構關鍵詞，強制轉為 DEPARTMENT\"\n",
    "\n",
    "            elif \"street\" in val_lower or \"road\" in val_lower or \"avenue\" in val_lower:\n",
    "                corrected_tag = \"STREET\"\n",
    "                reason = \"含街道關鍵詞\"\n",
    "            elif \"city\" in val_lower or \"town\" in val_lower or \"village\" in val_lower:\n",
    "                corrected_tag = \"CITY\"\n",
    "                reason = \"含城市關鍵詞\"\n",
    "            elif \"territory\" in val_lower or \"state\" in val_lower or (val.isupper() and len(val) <= 5):\n",
    "                corrected_tag = \"STATE\"\n",
    "                reason = \"含地區/州關鍵詞或疑似縮寫\"\n",
    "            elif self.zip_fullmatch_pattern.fullmatch(val.strip()):\n",
    "                corrected_tag = \"ZIP\"\n",
    "                reason = \"4碼郵遞區號\"\n",
    "\n",
    "            if corrected_tag != tag and reason and \"保持原標籤\" not in reason:\n",
    "                log.append(f\"{fid} - 🔁 {val}: {tag} → {corrected_tag}（{reason}）\")\n",
    "            elif \"保持原標籤\" in reason:\n",
    "                log.append(f\"{fid} - ⚠️ 跳過修正 {val}: {reason}\")\n",
    "\n",
    "        return corrected_tag\n",
    "\n",
    "# 主要執行\n",
    "extractor = OptimizedAddressExtractor()\n",
    "\n",
    "print(\"讀取檔案中...\")\n",
    "\n",
    "# === 讀取資料 ===\n",
    "try:\n",
    "    with open(submission_task1_answer, \"r\", encoding=\"utf-8\") as f:\n",
    "        task1_lines = f.readlines()\n",
    "\n",
    "    with open(submission_task2_answer_clean_invalid, \"r\", encoding=\"utf-8\") as f:\n",
    "        original_invalid_lines = [line.strip() for line in f.readlines()]\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 檔案讀取錯誤: {e}\")\n",
    "\n",
    "print(f\"待處理 Task1 資料: {len(task1_lines)} 筆\")\n",
    "print(f\"待處理無效標籤: {len(original_invalid_lines)} 筆\")\n",
    "\n",
    "# === 先存起來原本的 invalid 資料 ===\n",
    "print(\"存儲原本的 invalid 資料...\")\n",
    "original_invalid_data = []\n",
    "for line in original_invalid_lines:\n",
    "    original_invalid_data.append(line)\n",
    "\n",
    "# === 從 Task1 中擷取地址實體 ===\n",
    "print(\"從 Task1 擷取地址實體中...\")\n",
    "fid_entities = defaultdict(list)\n",
    "log = []\n",
    "\n",
    "processed_count = 0\n",
    "for line in task1_lines:\n",
    "    parts = line.strip().split('\\t')\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "\n",
    "    fid, sentence = parts\n",
    "    entities = extractor.extract_entities_from_sentence(fid, sentence, log)\n",
    "\n",
    "    if entities:\n",
    "        fid_entities[fid].extend(entities)\n",
    "\n",
    "    processed_count += 1\n",
    "\n",
    "    # 顯示進度\n",
    "    if processed_count % 5000 == 0:\n",
    "        progress = (processed_count / len(task1_lines)) * 100\n",
    "        print(f\"   進度: {processed_count}/{len(task1_lines)} ({progress:.1f}%)\")\n",
    "\n",
    "# === 開始組合最終結果：新抓到的放最前面 ===\n",
    "print(\"組合最終結果（新抓到的放前面）...\")\n",
    "final_lines = []\n",
    "\n",
    "# 1. 先加入從 Task1 新擷取的實體（放最前面）\n",
    "print(\"加入新擷取的實體...\")\n",
    "for fid, entities in fid_entities.items():\n",
    "    for tag, val in entities:\n",
    "        final_lines.append(f\"{fid}\\t{tag}\\t{val}\")\n",
    "\n",
    "# 2. 然後修正原本 invalid lines 的 LOCATION-OTHER / DISTRICT 並加入\n",
    "print(\"修正並加入原本的無效標籤...\")\n",
    "for line_data in original_invalid_data:\n",
    "    parts = line_data.strip().split('\\t')\n",
    "    if len(parts) != 3:\n",
    "        final_lines.append(line_data)\n",
    "        continue\n",
    "\n",
    "    fid, tag, val = parts\n",
    "    corrected_tag = extractor.correct_invalid_tag(tag, val, fid, log)\n",
    "    final_lines.append(f\"{fid}\\t{corrected_tag}\\t{val}\")\n",
    "\n",
    "# === 寫入檔案 ===\n",
    "print(\"寫入結果檔案...\")\n",
    "with open(submission_task2_answer_clean_invalid, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in final_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# === 統計與報告 ===\n",
    "new_entities_count = sum(len(v) for v in fid_entities.values())\n",
    "\n",
    "# 統計新實體類別\n",
    "category_stats = defaultdict(int)\n",
    "detailed_category_stats = defaultdict(int)\n",
    "\n",
    "for entities in fid_entities.values():\n",
    "    for tag, val in entities:\n",
    "        category_stats[tag] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"處理完成報告\")\n",
    "print(\"=\"*60)\n",
    "print(f\"最終總筆數: {len(final_lines)}\")\n",
    "print(f\"Task1 新增筆數: {new_entities_count}\")\n",
    "\n",
    "print(f\"\\n新增實體類別統計:\")\n",
    "for tag, count in sorted(category_stats.items()):\n",
    "    print(f\"   {tag}: {count} 筆\")\n",
    "\n",
    "# === 顯示處理日誌 ===\n",
    "if log:\n",
    "    print(f\"\\n處理日誌 (共 {len(log)} 筆):\")\n",
    "    print(\"-\" * 60)\n",
    "    for entry in log:\n",
    "        print(f\"   {entry}\")\n",
    "\n",
    "print(f\"\\n✅ 處理完成！結果已寫入: {submission_task2_answer_clean_invalid}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# ======== 正則表達式模式定義 ========\n",
    "\n",
    "# ---- MEDICAL_RECORD_NUMBER pattern ----\n",
    "MEDICAL_RECORD_NUMBER_patterns = [\n",
    "    r'\\b\\d{6,8}\\.?[A-Za-z]{2,5}\\.?\\b',  # 原始格式，例如 7533626.bkf\n",
    "]\n",
    "\n",
    "# ---- ID_NUMBER pattern ----\n",
    "ID_NUMBER_patterns = [\n",
    "    r'\\b\\d{2}[A-Z][A-Za-z0-9]{4,10}\\.?\\b',  # 原始格式\n",
    "\n",
    "    # 修正版：確保ID包含數字且不匹配純文字\n",
    "    r'\\b(?:Episode\\sNumber|Lab\\sNo\\.?|Lab\\sNumbers?|Laboratory\\sNumbers?|ID\\snumber|medical\\sepisode\\sidentified\\sas|identification\\snumbers?|episode|lab\\snumbers?\\sare)[:\\s]+(?P<id>(?=.*\\d)[A-Za-z0-9]{4,15})\\b'\n",
    "]\n",
    "\n",
    "\n",
    "# -------- DURATION 正則定義 --------\n",
    "number_words = [\n",
    "    \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "    \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "    \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\"\n",
    "]\n",
    "duration_units = [\n",
    "    \"secs\", \"seconds\", \"mins\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"\n",
    "]\n",
    "unit_pattern = \"|\".join(duration_units)\n",
    "number_pattern = rf\"(?:\\d+(?:\\.\\d+)?|{'|'.join(number_words)})\"\n",
    "DURATION_pattern = rf\"\\b{number_pattern}\\s?(?:{unit_pattern})(?!\\s?old)\\b\"\n",
    "\n",
    "# -------- AGE 正則定義 --------\n",
    "base_number_words = [\n",
    "    \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "    \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\",\n",
    "    \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\"\n",
    "]\n",
    "tens_words = [\"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "composite_number_words_dash = [f\"{tens}-{unit}\" for tens in tens_words for unit in base_number_words[:9]]\n",
    "composite_number_words_space = [f\"{tens} {unit}\" for tens in tens_words for unit in base_number_words[:9]]\n",
    "all_number_words = base_number_words + composite_number_words_dash + composite_number_words_space\n",
    "number_word_pattern = r\"(?:\" + \"|\".join(re.escape(word) for word in all_number_words) + r\")\"\n",
    "\n",
    "AGE_patterns = [\n",
    "    r\"\\b(?P<age1>\\d{1,3})(?:\\s|-)?(?:year|yr)s?(?:\\s|-)?old\\b\",\n",
    "    rf\"\\b(?P<age2>{number_word_pattern})(?:\\s|-)?(?:year|yr)s?(?:\\s|-)?old\\b\",\n",
    "    r\"\\b(?P<age3>\\d{2}s)\\b\",\n",
    "    rf\"\\b(?:he|she|they|i)\\s+(?:was|is|were|am)\\s+(?P<age4>(\\d{{1,3}}|{number_word_pattern}))(?!\\s+(of|among|amongst|between|the)\\b)\",\n",
    "    rf\"\\b(?:he's|she's|they're|i'm)\\s+(?P<age5>(\\d{{1,3}}|{number_word_pattern}))(?!\\s+(of|among|amongst|between|the)\\b)\"\n",
    "]\n",
    "\n",
    "# ======== TIME / ZIP / DOCTOR pattern ========\n",
    "TIME_patterns = [\n",
    "    r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\b',  # 24hr format\n",
    "    r'\\b(?:[01]?\\d|2[0-3])(?:[:.][0-5]\\d)?\\s?(?:a\\.?\\s?m\\.?|p\\.?\\s?m\\.?)\\b',  # am/pm\n",
    "    r\"\\b(?:[1-9]|1[0-2])\\s*o'?clock\\b\"  # e.g. 9 o'clock\n",
    "]\n",
    "\n",
    "\n",
    "ZIP_patterns = [\n",
    "    r'\\b(?P<zip>[A-Za-z]\\d[A-Za-z][ -]?\\d[A-Za-z]\\d)\\b',  # Canadian\n",
    "    r'\\b(?:ZIP\\s?code(?:\\s+of)?|Postal\\s?code(?:\\s+of)?):?\\s?(?P<zip>\\d{3,10})\\b',  # US style\n",
    "]\n",
    "\n",
    "DOCTOR_pattern = r'\\b(?:Dr\\.?|Doctor|Prof\\.?|Professor|Associate\\s+Professor)\\s+(?:[A-Z]\\.|\\b[A-Z][a-z]+)(?:\\s+[A-Z]\\.)?(?:\\s+[A-Z][a-z]+(?:-[A-Z][a-z]+)?)\\b'\n",
    "\n",
    "# === 先把原有 clean_invalid 的資料讀出來 ===\n",
    "with open(submission_task2_answer_clean_invalid, encoding=\"utf-8\") as f:\n",
    "    existing_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======== 統計與資料儲存變數 ========\n",
    "added_total = 0\n",
    "added_counter = {\n",
    "    \"MEDICAL_RECORD_NUMBER\": 0,\n",
    "    \"ID_NUMBER\": 0,\n",
    "    \"DURATION\": 0,\n",
    "    \"AGE\": 0,\n",
    "    \"TIME\": 0,\n",
    "    \"ZIP\": 0,\n",
    "    \"DOCTOR\": 0\n",
    "}\n",
    "added_records = []\n",
    "\n",
    "entity_category_mapping = {}\n",
    "with open(Validation_Dataset_Formal_entity, encoding=\"utf-8\") as f:\n",
    "    entities_data = json.load(f)\n",
    "    for item in entities_data:\n",
    "        for category, entities in item.items():\n",
    "            for entity in entities:\n",
    "                entity_category_mapping[entity] = category.upper()\n",
    "                entity_category_mapping[entity.lower()] = category.upper()\n",
    "\n",
    "# ======== 開始處理資料並抽取標註結果 ========\n",
    "with open(submission_task1_answer, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_clean_invalid, \"w\", encoding=\"utf-8\") as fout:  # 清空寫入\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "\n",
    "        fid, sentence = line.split(\"\\t\", 1)\n",
    "\n",
    "        # ---- MEDICAL_RECORD_NUMBER ----\n",
    "        for pattern in MEDICAL_RECORD_NUMBER_patterns:\n",
    "            for match in re.finditer(pattern, sentence, re.IGNORECASE):\n",
    "                entity = match.group('mrn') if 'mrn' in match.groupdict() else match.group()\n",
    "                label = \"MEDICAL_RECORD_NUMBER\"\n",
    "                fout.write(f\"{fid}\\t{label}\\t{entity}\\n\")\n",
    "                added_counter[label] += 1\n",
    "                added_total += 1\n",
    "                added_records.append((fid, label, entity))\n",
    "\n",
    "        # ---- ID_NUMBER ----\n",
    "        for pattern in ID_NUMBER_patterns:\n",
    "            for match in re.finditer(pattern, sentence, re.IGNORECASE):\n",
    "                entity = match.group('id') if 'id' in match.groupdict() else match.group()\n",
    "                label = \"ID_NUMBER\"\n",
    "                fout.write(f\"{fid}\\t{label}\\t{entity}\\n\")\n",
    "                added_counter[label] += 1\n",
    "                added_total += 1\n",
    "                added_records.append((fid, label, entity))\n",
    "\n",
    "        # ---- DURATION（排除 one day）----\n",
    "        full_matches = re.findall(DURATION_pattern, sentence, re.IGNORECASE)\n",
    "        filtered_matches = [match for match in full_matches if match.lower() != \"one day\"]\n",
    "        for match in filtered_matches:\n",
    "            label = \"DURATION\"\n",
    "            fout.write(f\"{fid}\\t{label}\\t{match}\\n\")\n",
    "            added_counter[label] += 1\n",
    "            added_total += 1\n",
    "            added_records.append((fid, label, match))\n",
    "\n",
    "        # ---- AGE ----\n",
    "        for pattern in AGE_patterns:\n",
    "            for match in re.finditer(pattern, sentence, flags=re.IGNORECASE):\n",
    "                for key in (\"age1\", \"age2\", \"age3\", \"age4\", \"age5\"):\n",
    "                    if key in match.groupdict() and match.group(key):\n",
    "                        age_value = match.group(key)\n",
    "                        label = \"AGE\"\n",
    "                        fout.write(f\"{fid}\\t{label}\\t{age_value}\\n\")\n",
    "                        added_counter[label] += 1\n",
    "                        added_total += 1\n",
    "                        added_records.append((fid, label, age_value))\n",
    "                        break\n",
    "\n",
    "        # ---- TIME ----\n",
    "        for pattern in TIME_patterns:\n",
    "            for match in re.finditer(pattern, sentence, re.IGNORECASE):\n",
    "                entity = match.group()\n",
    "                label = \"TIME\"\n",
    "                fout.write(f\"{fid}\\t{label}\\t{entity}\\n\")\n",
    "                added_counter[label] += 1\n",
    "                added_total += 1\n",
    "                added_records.append((fid, label, entity))\n",
    "\n",
    "        # ---- ZIP ----\n",
    "        for pattern in ZIP_patterns:\n",
    "            for match in re.finditer(pattern, sentence, re.IGNORECASE):\n",
    "                # 如果有 group dict（命名群組），取 zip；否則取整個 match\n",
    "                if isinstance(match, re.Match) and match.groupdict():\n",
    "                    zip_code = match.groupdict().get(\"zip\", match.group())\n",
    "                else:\n",
    "                    zip_code = match.group()\n",
    "\n",
    "                label = \"ZIP\"\n",
    "                fout.write(f\"{fid}\\t{label}\\t{zip_code}\\n\")\n",
    "                added_counter[label] += 1\n",
    "                added_total += 1\n",
    "                added_records.append((fid, label, zip_code))\n",
    "\n",
    "\n",
    "        # ---- DOCTOR ----\n",
    "        for match in re.findall(DOCTOR_pattern, sentence):\n",
    "            label = \"DOCTOR\"\n",
    "            fout.write(f\"{fid}\\t{label}\\t{match}\\n\")\n",
    "            added_counter[label] += 1\n",
    "            added_total += 1\n",
    "            added_records.append((fid, label, match))\n",
    "\n",
    "\n",
    "# ======== 將原本的 clean_invalid 清理後再寫回 ========\n",
    "removal_logs = []\n",
    "with open(submission_task2_answer_clean_invalid, \"a\", encoding=\"utf-8\") as fout:\n",
    "    for line in existing_lines:\n",
    "        if \"\\t\" not in line:\n",
    "            removal_logs.append((\"MissingTab\", line))\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) < 3:\n",
    "            removal_logs.append((\"TooFewParts\", line))\n",
    "            continue\n",
    "\n",
    "        fid = parts[0]\n",
    "        label = parts[1].strip().upper()\n",
    "        entity = \"\\t\".join(parts[2:]).strip()\n",
    "\n",
    "        entity_lower = entity.lower()\n",
    "        label_lower = label.lower()\n",
    "\n",
    "        if entity_lower == label_lower:\n",
    "            removal_logs.append((fid, label, entity, \"entity == label\"))\n",
    "            continue\n",
    "\n",
    "        tokens = entity_lower.split()\n",
    "        if len(tokens) == 2 and tokens[0] in {\"the\", \"a\", \"an\", \"this\", \"that\", \"those\"} and tokens[1] == label_lower:\n",
    "            removal_logs.append((fid, label, entity, \"entity = 冠詞+label\"))\n",
    "            continue\n",
    "\n",
    "        fout.write(f\"{fid}\\t{label}\\t{entity}\\n\")\n",
    "\n",
    "# === 顯示被過濾的項目 ===\n",
    "if removal_logs:\n",
    "    print(\"\\n被過濾項目：\")\n",
    "    for log in removal_logs:\n",
    "        print(f\"[{log[0]}] {log[1]} | {log[2]}  →  {log[3]}\")\n",
    "\n",
    "# ======== 印出統計結果 ========\n",
    "print(f\"\\n✅ 完成 MRN / ID / DURATION / AGE 抽取（已允許重複），結果已寫入：{submission_task2_answer_clean_invalid}\")\n",
    "print(f\"新增總筆數：{added_total} 筆\")\n",
    "print(f\"各類別新增統計：\")\n",
    "for label, count in added_counter.items():\n",
    "    print(f\"    {label}: {count} 筆\")\n",
    "\n",
    "print(\"\\n新增的資料列表：\")\n",
    "for fid, label, entity in added_records:\n",
    "    print(f\"{fid}\\t{label}\\t{entity}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 防範沒有出現過的東西出現在比賽中，先寫起來"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# ======== CONTACT & LOCATION-OTHER 修正版正則表達式 ========\n",
    "contact_patterns = {\n",
    "    \"EMAIL\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",\n",
    "    \"URL\": r\"\\b(?:https?://|www\\.)[a-zA-Z0-9\\-._~:/?#\\[\\]@!$&'()*+,;=%]+\\b\",\n",
    "    \"IPADDRESS\": r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\",\n",
    "    \"LOCATION-OTHER\": r\"\\b(?:P\\.?\\s?O\\.?\\s?BOX|GPO\\s?BOX|Locked\\s?Bag)[-\\s]?\\d+\\b\",\n",
    "}\n",
    "\n",
    "added_contact_total = 0\n",
    "added_contact_counter = {key: 0 for key in contact_patterns}\n",
    "added_contact_counter[\"ID_NUMBER\"] = 0  # 額外統計 ID_NUMBER（for PHONE 修正）\n",
    "added_contact_records = []\n",
    "\n",
    "# ======== 第一步：先讀取原本的 invalid 檔案內容（保留） ========\n",
    "with open(submission_task2_answer_clean_invalid, encoding=\"utf-8\") as f:\n",
    "    original_invalid_lines = f.readlines()\n",
    "\n",
    "# ======== 第二步：清空檔案並寫入新的 CONTACT 類資料 ========\n",
    "with open(submission_task1_answer, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_clean_invalid, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "\n",
    "        fid, sentence = line.split(\"\\t\", 1)\n",
    "\n",
    "        for label, pattern in contact_patterns.items():\n",
    "            for match in re.findall(pattern, sentence, flags=re.IGNORECASE):\n",
    "                fout.write(f\"{fid}\\t{label}\\t{match}\\n\")\n",
    "                added_contact_counter[label] += 1\n",
    "                added_contact_total += 1\n",
    "                added_contact_records.append((fid, label, match))\n",
    "\n",
    "# ======== 第三步：把原本內容 append 回檔案（修正 PHONE，排除 LOCATION-OTHER） ========\n",
    "with open(submission_task2_answer_clean_invalid, \"a\", encoding=\"utf-8\") as fout:\n",
    "    for line in original_invalid_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) == 3:\n",
    "            fid, label, value = parts\n",
    "\n",
    "            # 直接略過 LOCATION-OTHER\n",
    "            if label == \"LOCATION-OTHER\":\n",
    "                continue\n",
    "\n",
    "            # ✅ 修正 PHONE → ID_NUMBER\n",
    "            if label == \"PHONE\" and not re.fullmatch(r\"\\d{4}-\\d{4}\", value):\n",
    "                print(f\"📌 PHONE 改為 ID_NUMBER：{fid}\\t{label}\\t{value}\")\n",
    "                label = \"ID_NUMBER\"\n",
    "                added_contact_counter[\"ID_NUMBER\"] += 1\n",
    "\n",
    "            fout.write(f\"{fid}\\t{label}\\t{value}\\n\")\n",
    "        else:\n",
    "            fout.write(line + \"\\n\")\n",
    "\n",
    "# ======== 印出結果 ========\n",
    "print(f\"\\n📬 CONTACT / LOCATION-OTHER 修正抽取完成，新增總筆數：{added_contact_total}\")\n",
    "print(\"各類別統計：\")\n",
    "for label, count in added_contact_counter.items():\n",
    "    print(f\"    {label}: {count} 筆\")\n",
    "\n",
    "print(\"\\n📌 新增資料：\")\n",
    "for record in added_contact_records:\n",
    "    print(f\"{record[0]}\\t{record[1]}\\t{record[2]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 清理，並轉換人名相關類別"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# === 載入 entity 詞表 JSON ===\n",
    "with open(Validation_Dataset_Formal_entity, encoding=\"utf-8\") as f:\n",
    "    entities_data = json.load(f)\n",
    "\n",
    "# 建立 entity → 類別映射（大小寫都處理）\n",
    "entity_category_mapping = {}\n",
    "for item in entities_data:\n",
    "    for category, entities in item.items():\n",
    "        for entity in entities:\n",
    "            entity_category_mapping[entity.strip()] = category.upper()\n",
    "            entity_category_mapping[entity.strip().lower()] = category.upper()\n",
    "\n",
    "# === 清理與轉換邏輯 ===\n",
    "correction_logs = []\n",
    "conversion_count = defaultdict(int)  # 統計轉換次數\n",
    "\n",
    "with open(submission_task2_answer_clean_invalid, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_corrected, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "\n",
    "        fid, category, entity = parts\n",
    "        corrected_category = category.strip().upper()\n",
    "        entity = entity.strip()\n",
    "\n",
    "        # === 移除 <think> 標籤 ===\n",
    "        original_entity = entity\n",
    "        entity = re.sub(r\"</?think>\", \"\", entity, flags=re.IGNORECASE).strip()\n",
    "        if entity != original_entity:\n",
    "            correction_logs.append((fid, original_entity, corrected_category, corrected_category, f\"移除 <think> 標籤，轉為 {entity}\"))\n",
    "\n",
    "        # === 移除單獨 > 符號 ===\n",
    "        if \">\" in entity:\n",
    "            new_entity = entity.replace(\">\", \"\").strip()\n",
    "            if new_entity != entity:\n",
    "                correction_logs.append((fid, entity, corrected_category, corrected_category, f\"移除單獨的 '>' 符號，轉為 {new_entity}\"))\n",
    "                entity = new_entity\n",
    "\n",
    "        # === 檢查開頭為 Dr. 且類別不正確 → 強制轉為 DOCTOR\n",
    "        if corrected_category in {\"PATIENT\", \"FAMILYNAME\", \"PERSONALNAME\"}:\n",
    "            if re.match(r\"^dr\\.?\\s*\", entity, flags=re.IGNORECASE):\n",
    "                original_category = corrected_category\n",
    "                corrected_category = \"DOCTOR\"\n",
    "                correction_logs.append((fid, entity, original_category, corrected_category, \"開頭為 Dr.，原為人名類別 → 強制轉為 DOCTOR\"))\n",
    "\n",
    "        # === DOCTOR: 移除開頭的醫師或教授稱謂 ===\n",
    "        if corrected_category == \"DOCTOR\":\n",
    "            original_entity = entity\n",
    "\n",
    "            # 去除開頭職稱（不區分大小寫）\n",
    "            entity = re.sub(\n",
    "                r\"^(dr\\.?|prof\\.?|professor|associate professor)\\s*\",\n",
    "                \"\",\n",
    "                entity,\n",
    "                flags=re.IGNORECASE\n",
    "            ).strip()\n",
    "\n",
    "            if entity == \"\":\n",
    "                correction_logs.append((fid, original_entity, corrected_category, \"REMOVED\", \"DOCTOR 類別僅為稱謂，已移除\"))\n",
    "                continue\n",
    "            elif entity != original_entity:\n",
    "                correction_logs.append((fid, original_entity, corrected_category, corrected_category, f\"DOCTOR 去除前綴，轉為 {entity}\"))\n",
    "\n",
    "        # === ZIP: 只保留 4 位數\n",
    "        if corrected_category == \"ZIP\":\n",
    "            original_entity = entity\n",
    "            match = re.search(r\"\\b\\d{4}\\b\", entity)\n",
    "            if match:\n",
    "                entity = match.group(0)\n",
    "                if entity != original_entity:\n",
    "                    correction_logs.append((fid, original_entity, corrected_category, corrected_category, f\"ZIP 清理為 {entity}\"))\n",
    "            else:\n",
    "                correction_logs.append((fid, original_entity, corrected_category, \"REMOVED\", \"ZIP 類別無 4 位數郵遞區號，已移除\"))\n",
    "                continue\n",
    "\n",
    "        # === MRN / ID: 若出現在人名類別則轉換，否則保留 ===\n",
    "        if corrected_category in {\"MEDICAL_RECORD_NUMBER\", \"ID_NUMBER\"}:\n",
    "            lower_entity = entity.lower()\n",
    "            mapped_category = entity_category_mapping.get(lower_entity, \"\")\n",
    "            if mapped_category in {\"PATIENT\", \"FAMILYNAME\", \"PERSONALNAME\"}:\n",
    "                correction_logs.append((fid, entity, corrected_category, mapped_category, f\"{corrected_category} 出現在 {mapped_category} 類別，轉為 {mapped_category}\"))\n",
    "                conversion_count[(corrected_category, mapped_category)] += 1\n",
    "                corrected_category = mapped_category\n",
    "\n",
    "        # === PERSONALNAME: 若出現在 PATIENT 或 FAMILYNAME 詞表中則轉換 ===\n",
    "        if corrected_category == \"PERSONALNAME\":\n",
    "            lower_entity = entity.lower()\n",
    "            mapped_category = entity_category_mapping.get(lower_entity, \"\")\n",
    "            if mapped_category in {\"PATIENT\", \"FAMILYNAME\"}:\n",
    "                correction_logs.append((fid, entity, \"PERSONALNAME\", mapped_category, f\"PERSONALNAME 出現在 {mapped_category} 類別，轉為 {mapped_category}\"))\n",
    "                conversion_count[(\"PERSONALNAME\", mapped_category)] += 1\n",
    "                corrected_category = mapped_category\n",
    "\n",
    "        # === 寫入結果 ===\n",
    "        fout.write(f\"{fid}\\t{corrected_category}\\t{entity}\\n\")\n",
    "\n",
    "# === 顯示修正紀錄 ===\n",
    "if correction_logs:\n",
    "    print(\"\\n✅ 清理修正紀錄：\")\n",
    "    for log in correction_logs:\n",
    "        print(f\"[{log[0]}] {log[1]} → {log[3]}：{log[4]}\")\n",
    "\n",
    "# === 顯示轉換統計 ===\n",
    "if conversion_count:\n",
    "    print(\"\\n類別轉換統計：\")\n",
    "    for (from_cat, to_cat), count in sorted(conversion_count.items()):\n",
    "        print(f\"{from_cat} → {to_cat}: {count} 筆\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 規則一"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 時間單位定義\n",
    "ALL_UNITS = [\"minute\", \"hour\", \"day\", \"week\", \"weekend\", \"month\", \"year\",\n",
    "             \"night\", \"morning\", \"afternoon\", \"evening\"]\n",
    "WEEKDAYS = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "PLURAL_UNITS = [\"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\"]\n",
    "\n",
    "# 類別定義（全小寫）\n",
    "CATEGORY_PHRASES = {\n",
    "    \"SET\": [\n",
    "        f\"every {unit}\" for unit in ALL_UNITS + WEEKDAYS\n",
    "    ] + [\n",
    "        f\"{prefix} {unit}\" for prefix in [\"per\", \"each\", \"once a\", \"twice a\"] for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"{n} times a {unit}\" for n in range(3, 6) for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        \"couple of times\", \"daily\", \"weekly\", \"monthly\", \"yearly\",\n",
    "    ],\n",
    "\n",
    "    \"DURATION\": [\n",
    "        f\"several {unit}s\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"couple {unit}s\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"few {unit}s\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"past {unit}\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"past several {unit}s\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"past couple of {unit}s\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"past couple {unit}s\" for unit in PLURAL_UNITS\n",
    "    ] + [\n",
    "        f\"all {unit}\" for unit in ALL_UNITS\n",
    "    ] + [\n",
    "        f\"long {unit}\" for unit in ALL_UNITS\n",
    "    ] + [\n",
    "        f\"whole {unit}\" for unit in ALL_UNITS\n",
    "    ] + [\n",
    "        \"a short time\", \"a long time\", \"whole time\", \"short time\", \"long time\"\n",
    "    ],\n",
    "\n",
    "    \"DATE\": [\n",
    "        \"today\", \"yesterday\", \"tomorrow\",\n",
    "        \"this weekend\", \"next weekend\", \"last weekend\",\n",
    "        \"this week\", \"next week\", \"last week\",\n",
    "        \"this month\", \"next month\", \"last month\",\n",
    "        \"this year\", \"next year\", \"last year\",\n",
    "        \"right now\"\n",
    "    ],\n",
    "\n",
    "    \"TIME\": [\n",
    "        \"this morning\", \"this afternoon\", \"this evening\",\n",
    "        \"tonight\", \"last night\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# === 載入 submission_task1_answer ===\n",
    "with open(submission_task1_answer, encoding=\"utf-8\") as f_task1:\n",
    "    task1_lines = [line.strip().split(\"\\t\", 1) for line in f_task1 if \"\\t\" in line]\n",
    "\n",
    "# === 寫入 submission_task2_answer_rule1，並記錄抓取 ===\n",
    "rule1_logs = []\n",
    "\n",
    "with open(submission_task2_answer_rule1, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for fid, sentence in task1_lines:\n",
    "        lowered = sentence.lower()\n",
    "\n",
    "        for cat, phrase_list in CATEGORY_PHRASES.items():\n",
    "            for phrase in phrase_list:\n",
    "                pattern = rf\"\\b{re.escape(phrase)}\\b\"\n",
    "                matches = list(re.finditer(pattern, lowered))\n",
    "                for _ in matches:\n",
    "                    actual_phrase = \"now\" if cat == \"DATE\" and phrase == \"right now\" else phrase\n",
    "                    fout.write(f\"{fid}\\t{cat}\\t{actual_phrase}\\n\")\n",
    "                    rule1_logs.append((fid, cat, actual_phrase))\n",
    "\n",
    "    # === 接續寫入 corrected 的結果 ===\n",
    "    with open(submission_task2_answer_corrected, encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            fout.write(line)\n",
    "\n",
    "# === 顯示統計 ===\n",
    "stats = Counter(cat for _, cat, _ in rule1_logs)\n",
    "\n",
    "for cat in [\"SET\", \"DURATION\", \"DATE\", \"TIME\"]:\n",
    "    print(f\"{cat} 類別共抓取：{stats[cat]} 筆\")\n",
    "\n",
    "print(\"\\n詳細抓取清單：\")\n",
    "for fid, cat, phrase in rule1_logs:\n",
    "    print(f\"[{fid}] 抓取 {phrase} 為 {cat}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 轉換+清理 規則二"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 區域及組織相關互轉"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ======== 讀取 JSON 建立 entity 對應表（支援大小寫&中文處理） ========\n",
    "entity_category_mapping = {}\n",
    "# ===== 只對特定類別進行 JSON 白名單過濾，只針對以下類別進行強制補抓（高誤標風險 + JSON 可控）\n",
    "force_whitelist_categories = {\"PROFESSION\", \"LOCATION-OTHER\", \"DISTRICT\", \"COUNTY\", \"COUNTRY\"}\n",
    "group1_mutual_convert = {\"PROFESSION\", \"DEPARTMENT\", \"ORGANIZATION\", \"HOSPITAL\", \"LOCATION-OTHER\", \"DISTRICT\"}    # 可互轉類別，允許小寫比對\n",
    "group2_strict_match = {\"COUNTRY\", \"COUNTY\", \"STATE\", \"CITY\", \"STREET\", \"LOCATION-OTHER\", \"DISTRICT\",\n",
    "                       \"PROFESSION\", \"DEPARTMENT\", \"ORGANIZATION\", \"HOSPITAL\", \"LOCATION-OTHER\"}    # 需大小寫完全一致，互轉時需精準比對\n",
    "\n",
    "# ================================================================\n",
    "# group3_category_rules:\n",
    "# 定義時間相關類別（TIME / DATE / DURATION / SET）之間的語意修正邏輯\n",
    "# 此結構為 dict，每個 key 為「目標類別」，表示當條件滿足時，\n",
    "# 該 entity 原本的類別會被修正為此 key 所代表的類別。\n",
    "# 每個類別修正規則內包含：\n",
    "#   - trigger_categories: 哪些原始類別可被這個規則轉換\n",
    "#   - keywords: 觸發規則的關鍵詞集合\n",
    "#   - blocklist: 禁止轉換的詞（避免錯標，如 Sundays）\n",
    "#   - condition: lambda 函數，實際執行轉換邏輯\n",
    "#   - note: 說明此規則用途與語意\n",
    "# ================================================================\n",
    "group3_category_rules = {\n",
    "    \"SET\": {\n",
    "        \"trigger_categories\": {\"TIME\", \"DATE\", \"DURATION\"},\n",
    "        \"keywords\": {\"every\", \"once\", \"twice\", \"thrice\", \"each\", \"daily\", \"weekly\", \"times\"},\n",
    "        \"condition\": lambda entity: any(kw in entity.lower() for kw in group3_category_rules[\"SET\"][\"keywords\"]),\n",
    "        \"note\": \"Group3互轉：出現重複性時間詞 → 強制轉為 SET\"\n",
    "    },\n",
    "    \"DURATION\": {\n",
    "        \"trigger_categories\": {\"TIME\", \"DATE\"},\n",
    "        \"keywords\": {\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"},\n",
    "        \"blocklist\": {\n",
    "            \"sunday\", \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\",\n",
    "            \"weekdays\", \"weekends\", \"holidays\", \"days off\", \"sundays\", \"mondays\"\n",
    "        },\n",
    "        \"condition\": lambda entity: (\n",
    "            not any(bad in entity.lower() for bad in group3_category_rules[\"DURATION\"][\"blocklist\"])\n",
    "            and any(kw in entity.lower() for kw in group3_category_rules[\"DURATION\"][\"keywords\"])\n",
    "        ),\n",
    "        \"note\": \"Group3互轉：包含複數時間單位，且不在日曆詞黑名單中 → 強制轉為 DURATION\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ================================================================\n",
    "# group4_category_rules:\n",
    "# 針對組織相關類別（如 HOSPITAL、DEPARTMENT 等）建立基於關鍵詞的類別轉換規則。\n",
    "# 結構與 group3_category_rules 相同，每個 key 為「目標類別」，代表當觸發條件滿足時，\n",
    "# 將原本的標註類別強制轉換為此目標類別。\n",
    "#\n",
    "# 每個類別規則中包含：\n",
    "#   - trigger_categories: 可被此規則轉換的原始類別集合\n",
    "#   - keywords: 若 entity 中出現這些關鍵詞，則視為符合條件\n",
    "#   - condition: lambda 函數，實際執行判斷 entity 是否命中關鍵詞\n",
    "#   - note: 對此規則的說明，會記錄進 correction_logs 供分析與除錯\n",
    "# 因為group4會remove，所以必須把自身算上，剛group3不同的點在這\n",
    "\n",
    "group4_category_rules = {\n",
    "    \"DEPARTMENT\": {\n",
    "        \"trigger_categories\": {\"HOSPITAL\", \"DEPARTMENT\", \"ORGANIZATION\",  \"LOCATION-OTHER\", \"ROOM\"},\n",
    "        \"keywords\": {\"department\", \"unit\", \"ward\", \"division\", \"section\", \"clinic\", \"rooms\", \"central\", \"pathology\"},\n",
    "        \"condition\": lambda entity: any(kw in entity.lower() for kw in group4_category_rules[\"DEPARTMENT\"][\"keywords\"]),\n",
    "        \"note\": \"Group4互轉：包含部門單位關鍵詞 → 強制轉為 DEPARTMENT\"\n",
    "    },\n",
    "\n",
    "    \"HOSPITAL\": {\n",
    "        \"trigger_categories\": {\"HOSPITAL\", \"DEPARTMENT\", \"ORGANIZATION\", \"LOCATION-OTHER\", \"PROFESSION\", \"ROOM\"},\n",
    "        \"keywords\": {\"hospital\", \"medical center\", \"health\", \"healthy\", \"health center\", \"service\", \"centre\", \"center\"},\n",
    "        \"condition\": lambda entity: any(kw in entity.lower() for kw in group4_category_rules[\"HOSPITAL\"][\"keywords\"]),\n",
    "        \"note\": \"Group4互轉：包含醫療機構關鍵詞 → 強制轉為 HOSPITAL\"\n",
    "    },\n",
    "}\n",
    "\n",
    "group1_entities = {}  # Group1 對應的實體詞表（純小寫），用於後續硬抓比對\n",
    "group2_entities = {}  # Group2 對應的實體詞表（大小寫需相符），用於後續硬抓比對\n",
    "\n",
    "with open(Validation_Dataset_Formal_entity, encoding=\"utf-8\") as f:\n",
    "    entities_data = json.load(f)\n",
    "    for item in entities_data:\n",
    "        for category, entities in item.items():\n",
    "            for entity in entities:\n",
    "                entity_category_mapping[entity] = category.upper()\n",
    "                entity_category_mapping[entity.lower()] = category.upper()\n",
    "                if category.upper() in group1_mutual_convert:\n",
    "                    group1_entities.setdefault(category.upper(), set()).add(entity.lower())\n",
    "                if category.upper() in group2_strict_match:\n",
    "                    group2_entities.setdefault(category.upper(), set()).add(entity)\n",
    "\n",
    "# ======== 類別 TIME → DATE 的日曆詞關鍵字，用於特殊轉換 ========\n",
    "date_like_keywords = {\n",
    "    \"today\", \"tomorrow\", \"yesterday\", \"now\",\n",
    "    \"this week\", \"last week\", \"next week\",\n",
    "    \"this month\", \"last month\", \"next month\",\n",
    "    \"this year\", \"last year\", \"next year\",\n",
    "    \"this weekend\", \"last weekend\", \"next weekend\",\n",
    "    \"weekend\", \"week\",\n",
    "    \"monday\", \"tuesday\", \"wednesday\", \"thursday\",\n",
    "    \"friday\", \"saturday\", \"sunday\", \"sundays\",\n",
    "    \"easter\", \"same day\", \"same date\",\n",
    "    \"may\", \"june\", \"july\", \"august\", \"september\",\n",
    "    \"october\", \"november\", \"december\", \"january\", \"february\",\n",
    "    \"march\", \"april\"\n",
    "}\n",
    "\n",
    "# ======== 類別 DATE → TIME 的時間詞關鍵字，用於特殊轉換 ========\n",
    "time_like_keywords = {\n",
    "    \"morning\", \"mornings\", \"late morning\", \"early morning\", \"midmorning\", \"mid-morning\",\n",
    "    \"afternoon\", \"afternoons\", \"late afternoon\", \"early afternoon\", \"mid-afternoon\",\n",
    "    \"evening\", \"evenings\", \"late evening\", \"early evening\",\n",
    "    \"night\", \"nights\", \"early night\", \"midnight\", \"tonight\",\n",
    "    \"noon\", \"midday\", \"last light\", \"this morning\", \"yesterday morning\", \"tomorrow night\",\n",
    "}\n",
    "\n",
    "# ======== 校正過程：修正類別，並不做去重，保留所有預測項目 ========\n",
    "correction_logs = []\n",
    "\n",
    "with open(submission_task2_answer_rule1, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_rule2, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "\n",
    "        fid, category, entity = parts\n",
    "        category = category.strip().upper()\n",
    "        entity = entity.strip()\n",
    "        corrected_category = category\n",
    "\n",
    "        # ======== 類別標籤修正映射表（處理常見誤標） ========\n",
    "        category_correction_map = {\n",
    "            \"LAB_NUMBER\": \"ID_NUMBER\",\n",
    "            \"ADDRESS\": \"STREET\",\n",
    "            \"DAY\": \"DATE\",\n",
    "            \"DATETIME\": \"DATE\",\n",
    "            # \"MONTH\": \"DATE\",\n",
    "            # \"TIMEPOINT\": \"TIME\",\n",
    "            # \"YEARS\": \"DURATION\",  # 目前沒發現這種\n",
    "        }\n",
    "\n",
    "        # 轉換常見誤標的類別\n",
    "        if corrected_category in category_correction_map:\n",
    "            new_cat = category_correction_map[corrected_category]\n",
    "            correction_logs.append((fid, entity, corrected_category, new_cat, f\"誤標為 {corrected_category}，轉為 {new_cat}\"))\n",
    "            corrected_category = new_cat\n",
    "\n",
    "        # ===== FAMILYNAME / PERSONALNAME 非對稱修正邏輯（僅允許白名單進入 FAMILYNAME） =====\n",
    "        mapped = entity_category_mapping.get(entity) or entity_category_mapping.get(entity.lower())\n",
    "\n",
    "        # 若模型標成 FAMILYNAME，但 JSON 詞表中實際不是 FAMILYNAME → 強轉為 PERSONALNAME\n",
    "        if category == \"FAMILYNAME\" and mapped != \"FAMILYNAME\":\n",
    "            correction_logs.append((fid, entity, category, \"PERSONALNAME\", \"FAMILYNAME → PERSONALNAME\"))\n",
    "            corrected_category = \"PERSONALNAME\"\n",
    "\n",
    "        # 若模型標成 PERSONALNAME，但 JSON 詞表中實際是 FAMILYNAME → 強轉為 FAMILYNAME\n",
    "        elif category == \"PERSONALNAME\" and mapped == \"FAMILYNAME\":\n",
    "            correction_logs.append((fid, entity, category, \"FAMILYNAME\", \"PERSONALNAME → FAMILYNAME\"))\n",
    "            corrected_category = \"FAMILYNAME\"\n",
    "\n",
    "        for target_cat, rule in group3_category_rules.items():\n",
    "            if corrected_category in rule[\"trigger_categories\"]:\n",
    "                if rule[\"condition\"](entity):\n",
    "                    # 嘗試找出觸發規則的關鍵詞（若 rule 中有 keywords）\n",
    "                    matched_kw = None\n",
    "                    if \"keywords\" in rule:\n",
    "                        for kw in rule[\"keywords\"]:\n",
    "                            if kw in entity.lower():\n",
    "                                matched_kw = kw\n",
    "                                break\n",
    "                    # 組成 log 訊息\n",
    "                    reason = f\"{rule['note']}（關鍵詞: '{matched_kw}'）\" if matched_kw else rule[\"note\"]\n",
    "                    correction_logs.append((fid, entity, corrected_category, target_cat, reason))\n",
    "                    corrected_category = target_cat\n",
    "\n",
    "        # ===== 如果是 TIME 且為日曆詞 → 強制轉為 DATE =====\n",
    "        if corrected_category == \"TIME\" and entity.lower() in date_like_keywords:\n",
    "            correction_logs.append((fid, entity, corrected_category, \"DATE\", \"包含日曆詞，TIME → DATE\"))\n",
    "            corrected_category = \"DATE\"\n",
    "\n",
    "        # ===== 如果是 DATE 且含時間詞 → 強制轉為 TIME =====\n",
    "        if corrected_category == \"DATE\" and any(k in entity.lower() for k in time_like_keywords):\n",
    "            correction_logs.append((fid, entity, corrected_category, \"TIME\", \"包含時間詞，DATE → TIME\"))\n",
    "            corrected_category = \"TIME\"\n",
    "\n",
    "        # === 自動拆 CITY + ZIP 規則擴充 ===\n",
    "        extra_entities = []\n",
    "        if category == \"CITY\" and re.search(r\"\\b\\d{4}\\b\", entity):\n",
    "            zip_match = re.search(r\"\\b(\\d{4})\\b\", entity)\n",
    "            if zip_match:\n",
    "                zip_code = zip_match.group(1)\n",
    "                entity_city = entity.replace(zip_code, \"\").strip(\" ,\")\n",
    "                correction_logs.append((fid, entity, category, \"ZIP\", f\"CITY 含 4 位數字 ZIP，拆分 → CITY + ZIP（ZIP: {zip_code}）\"))\n",
    "                # 加入 ZIP 為新項目\n",
    "                extra_entities.append((\"ZIP\", zip_code))\n",
    "                # 修改原本這筆的 entity 為純 CITY（不含 ZIP）\n",
    "                entity = entity_city\n",
    "\n",
    "        # ===== Group1 小寫互轉 =====\n",
    "        converted_by_group1 = False\n",
    "        if corrected_category in group1_entities:\n",
    "            mapped_category = entity_category_mapping.get(entity.lower())\n",
    "            if (\n",
    "                mapped_category\n",
    "                and mapped_category in group1_entities\n",
    "                and mapped_category != corrected_category\n",
    "            ):\n",
    "                correction_logs.append((fid, entity, corrected_category, mapped_category, \"Group1互轉：依 JSON 詞表修正\"))\n",
    "                corrected_category = mapped_category\n",
    "                converted_by_group1 = True\n",
    "\n",
    "        # ===== Group2 精確大小寫互轉 =====\n",
    "        converted_by_group2 = False\n",
    "        if not converted_by_group1 and corrected_category in group2_entities:\n",
    "            mapped_category = entity_category_mapping.get(entity)\n",
    "            if (\n",
    "                mapped_category\n",
    "                and mapped_category in group2_entities\n",
    "                and mapped_category != corrected_category\n",
    "            ):\n",
    "                correction_logs.append((fid, entity, corrected_category, mapped_category, \"Group2互轉：依 JSON 詞表修正\"))\n",
    "                corrected_category = mapped_category\n",
    "                converted_by_group2 = True\n",
    "\n",
    "        # ===== force_whitelist 類別詞表比對（不立刻移除）=====\n",
    "        in_whitelist_by_group1 = (\n",
    "            corrected_category in group1_entities and entity.lower() in group1_entities[corrected_category]\n",
    "        )\n",
    "        in_whitelist_by_group2 = (\n",
    "            corrected_category in group2_entities and entity in group2_entities[corrected_category]\n",
    "        )\n",
    "\n",
    "        # ===== \"X of Y\" 拆解邏輯 =====\n",
    "        if \" of \" in entity.lower():\n",
    "            keyword_hits = set()\n",
    "            entity_lc = entity.lower()\n",
    "            for cat, rule in group4_category_rules.items():\n",
    "                if rule[\"condition\"](entity_lc):\n",
    "                    keyword_hits.add(cat)\n",
    "\n",
    "            if len(keyword_hits) >= 2:\n",
    "                for connector in [\"of\", \"and\"]:\n",
    "                    parts = re.split(rf'\\s+{connector}\\s+', entity, flags=re.IGNORECASE)\n",
    "                    if len(parts) == 2:\n",
    "                        left_raw, right_raw = parts\n",
    "                        left_part = left_raw.strip()\n",
    "                        right_part = right_raw.strip()\n",
    "\n",
    "                        left_cat, right_cat = None, None\n",
    "                        for cat, rule in group4_category_rules.items():\n",
    "                            if rule[\"condition\"](left_part.lower()):\n",
    "                                left_cat = cat\n",
    "                            if rule[\"condition\"](right_part.lower()):\n",
    "                                right_cat = cat\n",
    "\n",
    "                        if left_cat and right_cat:\n",
    "                            fout.write(f\"{fid}\\t{left_cat}\\t{left_part}\\n\")\n",
    "                            fout.write(f\"{fid}\\t{right_cat}\\t{right_part}\\n\")\n",
    "                            correction_logs.append(\n",
    "                                (fid, entity, category, f\"{left_cat} + {right_cat}\",\n",
    "                                 f\"雙詞拆分處理{connector}: left={left_cat}, right={right_cat}\")\n",
    "                            )\n",
    "                            continue  # ✅ 保持原來的邏輯，處理完這一筆就跳下一筆\n",
    "\n",
    "         # ===== Group4 keyword-based 類別轉換（照 dict 順序跑一輪，允許多次覆蓋，保留最後一個命中）=====\n",
    "        converted_by_group4 = False\n",
    "\n",
    "        # ===== 特例：若 entity 含 \"department\" → 強制轉為 DEPARTMENT =====\n",
    "        if \"department\" in entity.lower():\n",
    "            if corrected_category != \"DEPARTMENT\":\n",
    "                correction_logs.append((fid, entity, corrected_category, \"DEPARTMENT\", \"包含 'department' 字串 → 強制轉為 DEPARTMENT\"))\n",
    "            corrected_category = \"DEPARTMENT\"\n",
    "            converted_by_group4 = True\n",
    "        # ===== Group4 keyword-based 類別轉換（照 dict 順序跑一輪，允許多次覆蓋，保留最後一個命中）=====\n",
    "\n",
    "        # 如果不是特例才進入一般的 Group4 類別轉換\n",
    "        if not converted_by_group4:\n",
    "            for target_cat, rule in group4_category_rules.items():\n",
    "                if corrected_category in rule[\"trigger_categories\"]:\n",
    "                    if rule[\"condition\"](entity):\n",
    "                        old_cat = corrected_category\n",
    "                        corrected_category = target_cat\n",
    "                        reason = rule[\"note\"]\n",
    "                        correction_logs.append((fid, entity, old_cat, target_cat, reason))\n",
    "                        converted_by_group4 = True\n",
    "\n",
    "        # ===== 最終篩選：如果是 force_whitelist 類別，且未被任何邏輯保留 → 移除 =====\n",
    "        if corrected_category in group1_mutual_convert:\n",
    "            if not (\n",
    "                converted_by_group1\n",
    "                or converted_by_group2\n",
    "                or converted_by_group4\n",
    "                or in_whitelist_by_group1\n",
    "                or in_whitelist_by_group2\n",
    "            ):\n",
    "                # 雖然未命中，但是就保留，比賽時使用\n",
    "                correction_logs.append((fid, entity, corrected_category, \"比賽保留\", \"未命中任何條件\"))\n",
    "\n",
    "                # 所有東西未命中就刪除\n",
    "                # correction_logs.append((fid, entity, corrected_category, \"REMOVED\", \"未命中任何條件，移除\"))\n",
    "                # continue\n",
    "\n",
    "        # ===== 最終寫入，保留所有標註，不去重 =====\n",
    "        fout.write(f\"{fid}\\t{corrected_category}\\t{entity}\\n\")\n",
    "\n",
    "# ======== 顯示修正結果 ========\n",
    "print(f\"✅ 分類校正完成，基本結果寫入：{submission_task2_answer_rule2}\")\n",
    "print(f\"⚡ 總共修正錯誤筆數：{len(correction_logs)}\")\n",
    "if correction_logs:\n",
    "    print(\"修正清單（fid, entity, 原本類別, 修正後類別, 原因）：\")\n",
    "    for log in correction_logs:\n",
    "        print(log)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 人名相關"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# ========= PERSONALNAME → PATIENT / DOCTOR 語境修正邏輯 =========\n",
    "medical_trigger_categories = {\"DOCTOR\", \"HOSPITAL\", \"MEDICAL_RECORD_NUMBER\", \"ID_NUMBER\"}\n",
    "patient_logs = []\n",
    "\n",
    "# 定義縮寫人名判斷函數\n",
    "def is_short_initial_name(entity):\n",
    "    entity = entity.strip()\n",
    "    if re.fullmatch(r\"([A-Za-z]\\.\\s?){1,2}\", entity) or re.fullmatch(r\"[A-Za-z]{2}\", entity):\n",
    "        return True\n",
    "    if re.fullmatch(r\"([A-Za-z]\\.\\s?){1,2}[A-Za-z][a-z]+\", entity):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# === 讀取 JSON 裡的 PERSONALNAME 作為白名單 ===\n",
    "json_personalnames = set()\n",
    "with open(Validation_Dataset_Formal_entity, encoding=\"utf-8\") as f:\n",
    "    raw_entities = json.load(f)\n",
    "    for item in raw_entities:\n",
    "        for category, entities in item.items():\n",
    "            if category.upper() == \"PERSONALNAME\":\n",
    "                for ent in entities:\n",
    "                    json_personalnames.add(ent.strip())\n",
    "\n",
    "# === 第一步：掃描每個 fid 是否觸發醫療語境 ===\n",
    "fid_triggered = {}\n",
    "with open(submission_task2_answer_rule2, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "        fid, category, entity = line.split(\"\\t\")\n",
    "        if category.upper() in medical_trigger_categories and fid not in fid_triggered:\n",
    "            fid_triggered[fid] = category.upper()\n",
    "\n",
    "# === 第二步：PERSONALNAME 根據條件轉換 ===\n",
    "new_lines = []\n",
    "with open(submission_task2_answer_rule2, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "        fid, category, entity = line.split(\"\\t\")\n",
    "        original_category = category.strip().upper()\n",
    "        entity = entity.strip()\n",
    "\n",
    "        # 保留白名單中的 PERSONALNAME 不做更改\n",
    "        if original_category == \"PERSONALNAME\" and entity in json_personalnames:\n",
    "            new_lines.append(f\"{fid}\\t{original_category}\\t{entity}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 判斷縮寫 → DOCTOR\n",
    "        if original_category == \"PERSONALNAME\" and is_short_initial_name(entity):\n",
    "            reason = f\"姓名縮寫，PERSONALNAME → DOCTOR\"\n",
    "            new_lines.append(f\"{fid}\\tDOCTOR\\t{entity}\\n\")\n",
    "            patient_logs.append((fid, entity, \"PERSONALNAME\", \"DOCTOR\", reason))\n",
    "\n",
    "        # 判斷醫療語境 → PATIENT\n",
    "        elif original_category == \"PERSONALNAME\" and fid in fid_triggered:\n",
    "            reason = f\"{fid} 中有出現過 {fid_triggered[fid]}，PERSONALNAME → PATIENT\"\n",
    "            new_lines.append(f\"{fid}\\tPATIENT\\t{entity}\\n\")\n",
    "            patient_logs.append((fid, entity, \"PERSONALNAME\", \"PATIENT\", reason))\n",
    "\n",
    "        elif original_category == \"PERSONALNAME\":\n",
    "            reason = f\"PERSONALNAME → PATIENT\"\n",
    "            new_lines.append(f\"{fid}\\tPATIENT\\t{entity}\\n\")\n",
    "            patient_logs.append((fid, entity, \"PERSONALNAME\", \"PATIENT\", reason))\n",
    "\n",
    "        else:\n",
    "            new_lines.append(f\"{fid}\\t{original_category}\\t{entity}\\n\")\n",
    "\n",
    "# === 寫回檔案 ===\n",
    "with open(submission_task2_answer_rule2, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.writelines(new_lines)\n",
    "\n",
    "# === 將 JSON 裡的 PERSONALNAME 寫入 ===\n",
    "with open(submission_task1_answer, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_rule2, \"a\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        if \"\\t\" not in line:\n",
    "            continue\n",
    "        fid, sentence = line.strip().split(\"\\t\", 1)\n",
    "\n",
    "        for name in json_personalnames:\n",
    "            if name in sentence:\n",
    "                fout.write(f\"{fid}\\tPERSONALNAME\\t{name}\\n\")\n",
    "                print(f\"[{fid}] 補寫 PERSONALNAME：{name}\")\n",
    "\n",
    "\n",
    "# === 顯示 log ===\n",
    "print(f\"✅ PERSONALNAME 語意修正完成（含縮寫 → DOCTOR），處理筆數：{len(patient_logs)}\")\n",
    "if patient_logs:\n",
    "    print(\"修正清單（fid, entity, 原本類別, 修正後類別, 原因）：\")\n",
    "    for log in patient_logs:\n",
    "        print(log)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##　直接抓取Group1+Group2 規則二"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# ======== 額外從 task1 強制抓取指定類別的 entity，只要出現就寫入，並插入到 corrected 最前面 ========\n",
    "\n",
    "# 新增補抓總數與記錄\n",
    "forced_added = 0\n",
    "forced_added_records = []\n",
    "\n",
    "# ======== 建立 group2_entities（來自原始 JSON） ========\n",
    "group2_entities = {}\n",
    "with open(Validation_Dataset_Formal_entity, encoding=\"utf-8\") as f:\n",
    "    raw_entities = json.load(f)\n",
    "    for item in raw_entities:\n",
    "        for category, entities in item.items():\n",
    "            cat_upper = category.upper()\n",
    "            if cat_upper in force_whitelist_categories:\n",
    "                group2_entities.setdefault(cat_upper, []).extend(entities)\n",
    "\n",
    "# ======== 暫存原本 corrected 內容 ========\n",
    "with open(submission_task2_answer_rule2, \"r\", encoding=\"utf-8\") as fin:\n",
    "    original_lines = fin.readlines()\n",
    "\n",
    "# ======== 開啟 corrected，先寫補抓，再接上原內容 ========\n",
    "with open(submission_task2_answer_rule2, \"w\", encoding=\"utf-8\") as fout, \\\n",
    "     open(submission_task1_answer, encoding=\"utf-8\") as fin_task1:\n",
    "\n",
    "    for line in fin_task1:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "\n",
    "        fid, sentence = line.split(\"\\t\", 1)\n",
    "        sentence_lower = sentence.lower()\n",
    "        sentence_cleaned = re.sub(r\"[^\\w\\s]\", \" \", sentence).lower()  # 清除標點進行寬鬆比對\n",
    "\n",
    "        # ===== Group1：小寫比對詞幹（詞邊界 + 's） =====\n",
    "        for forced_cat, entity_set in group1_entities.items():\n",
    "            if forced_cat not in force_whitelist_categories:\n",
    "                continue\n",
    "            for entity in entity_set:\n",
    "                pattern = rf\"\\b{re.escape(entity)}(?=\\b|'s|\\s|$)\"\n",
    "                matches = re.findall(pattern, sentence_lower)\n",
    "                for _ in matches:\n",
    "                    fout.write(f\"{fid}\\t{forced_cat}\\t{entity}\\n\")\n",
    "                    forced_added_records.append((fid, forced_cat, entity, \"Group1補抓（小寫詞幹比對）\"))\n",
    "                    forced_added += 1\n",
    "\n",
    "        # ===== Group2：清除標點後小寫比對詞幹 =====\n",
    "        for forced_cat, entity_list in group2_entities.items():\n",
    "            if forced_cat not in force_whitelist_categories:\n",
    "                continue\n",
    "            for entity in entity_list:\n",
    "                pattern = rf\"\\b{re.escape(entity.lower())}(?=\\b|'s|\\s|$)\"\n",
    "                matches = re.findall(pattern, sentence_cleaned)\n",
    "                for _ in matches:\n",
    "                    fout.write(f\"{fid}\\t{forced_cat}\\t{entity}\\n\")\n",
    "                    forced_added_records.append((fid, forced_cat, entity, \"Group2補抓（清除標點小寫詞幹比對）\"))\n",
    "                    forced_added += 1\n",
    "\n",
    "\n",
    "    # ===== 接續原本的模型或校正內容 =====\n",
    "    fout.writelines(original_lines)\n",
    "\n",
    "# debug\n",
    "for i in force_whitelist_categories:\n",
    "    print(f\"[CHECK] group2_entities['{i}'] size = {len(group2_entities.get(i, []))}\")\n",
    "\n",
    "# ======== 顯示強制新增結果 ========\n",
    "print(f\"\\n✅ 額外強制抓取完成，共新增 {forced_added} 筆來自 task1 的 entity！\")\n",
    "if forced_added_records:\n",
    "    print(\"📌 強制新增的清單（fid, 類別, entity, 補抓依據）：\")\n",
    "    for record in forced_added_records:\n",
    "        print(record)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 強制改標籤"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === 讀取原始檔案 ===\n",
    "with open(submission_task2_answer_rule2, encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "\n",
    "fixed_lines = []\n",
    "change_log = []\n",
    "delete_counter = Counter()\n",
    "converted_count = 0\n",
    "district_to_street_count = 0\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line or \"\\t\" not in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 3:\n",
    "        fixed_lines.append(line)\n",
    "        continue\n",
    "\n",
    "    fid, label, entity = parts\n",
    "    label_upper = label.upper()\n",
    "    entity_lower = entity.strip().lower()\n",
    "\n",
    "    # === 修正：COUNTRY → STREET\n",
    "    if label_upper == \"COUNTRY\" and entity_lower == \"wales\":\n",
    "        change_log.append(f\"{fid} - COUNTRY → STREET (entity = {entity})\")\n",
    "        label = \"STREET\"\n",
    "        converted_count += 1\n",
    "\n",
    "    # === 修正：DISTRICT → STREET（無條件）\n",
    "    elif label_upper == \"DISTRICT\":\n",
    "        change_log.append(f\"{fid} - DISTRICT → STREET (entity = {entity})\")\n",
    "        label = \"STREET\"\n",
    "        district_to_street_count += 1\n",
    "\n",
    "    fixed_lines.append(f\"{fid}\\t{label}\\t{entity}\")\n",
    "\n",
    "# === 寫入修正後的結果 ===\n",
    "with open(submission_task2_answer_rule2, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fixed_lines:\n",
    "        fout.write(line + \"\\n\")\n",
    "\n",
    "# === 動態 log 輸出 ===\n",
    "print(f\"✅ 修正完成：\")\n",
    "print(f\"  COUNTRY → STREET 的修正筆數：{converted_count}\")\n",
    "print(f\"  DISTRICT → STREET 的修正筆數：{district_to_street_count}\")\n",
    "print(f\"  含數字而被移除的筆數：{sum(delete_counter.values())}\")\n",
    "for label, count in delete_counter.items():\n",
    "    print(f\"    {label}: {count} 筆\")\n",
    "\n",
    "print(\"\\n修正/移除紀錄：\")\n",
    "for entry in change_log:\n",
    "    print(f\"  {entry}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 刪除不必要的entity"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ======== 合法類別 ========\n",
    "valid_categories = {\n",
    "    'PATIENT', 'DOCTOR', 'USERNAME', 'FAMILYNAME', 'PERSONALNAME', 'PROFESSION',\n",
    "    'ROOM', 'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET', 'CITY',\n",
    "    'DISTRICT', 'COUNTY', 'STATE', 'COUNTRY', 'ZIP', 'LOCATION-OTHER',\n",
    "    'AGE', 'DATE', 'TIME', 'DURATION', 'SET', 'PHONE', 'FAX', 'EMAIL',\n",
    "    'URL', 'IPADDRESS', 'SOCIAL_SECURITY_NUMBER', 'MEDICAL_RECORD_NUMBER',\n",
    "    'HEALTH_PLAN_NUMBER', 'ACCOUNT_NUMBER', 'LICENSE_NUMBER', 'VEHICLE_ID',\n",
    "    'DEVICE_ID', 'BIOMETRIC_ID', 'ID_NUMBER', 'OTHER'\n",
    "}\n",
    "\n",
    "# ======== 容易被誤標的類別 ========\n",
    "# entity是「代名詞」或「模糊描述」會被視為錯誤\n",
    "invalid_categories = {\n",
    "    'DOCTOR',          # 很容易錯抓像 \"he\", \"she\" 或 \"someone\"\n",
    "    'PATIENT',         # 常被誤抓為家庭成員稱謂（e.g., mom, dad）\n",
    "    'FAMILYNAME',      # 容易抓到代詞或常見詞（e.g., mine, he）\n",
    "    'PERSONALNAME',    # 容易混入非人名\n",
    "    'PROFESSION',      # 容易誤抓 \"psychiatrist\"、\"friend\" 等模糊描述\n",
    "    'DURATION'         # 容易誤抓成無意義時間長度詞\n",
    "}\n",
    "\n",
    "# ======== 會被視為無效entity的「代名詞」或泛稱詞彙表 ========\n",
    "# 如果直接出現在 invalid_categories 對應類別的entity中，就直接移除\n",
    "invalid_pronouns = {\n",
    "    # 人稱代名詞\n",
    "    \"his\", \"him\", \"her\", \"he\", \"she\", \"i\", \"mine\", \"you\", \"they\", \"their\", \"that\", \"husband\", \"wife\",\n",
    "    \"myself\", \"yourself\", \"dr\", \"dr.\",\n",
    "    # 模糊性別描述或人類泛稱\n",
    "    \"male\", \"female\",\n",
    "    # 不屬於特定身份的角色\n",
    "    \"phone\", \"psychiatrist\", \"psychiatrists\",\n",
    "    \"doctor\", \"doctors\", \"anesthetist\",\n",
    "    \"gp\", \"professor\", \"associate professor\",\n",
    "    # 容易混淆的詞\n",
    "    \"age\", \"city\", \"early\", \"supercuts\", \"facebook\", \"regular\", \"long\", \"short\",\n",
    "    \"medical\", \"profession\", \"professions\", \"anesthesiologist\", \"nurse\", \"professional\"\n",
    "}\n",
    "\n",
    "# 適用於非核心類別時的排除條件，用來過濾模型誤抓的entity詞\n",
    "# ======== 模糊詞彙列表（分為人物類別與其他類別） ========\n",
    "# 人物相關模糊詞彙\n",
    "person_keywords = {\n",
    "    \"friend\", \"started\", \"shrink\", \"younger\", \"older\", \"you\", \"my\", \"your\",\n",
    "    \"someone\", \"mom\", \"dad\", \"parent\", \"husband\", \"sister\", \"brother\", \"daughter\", \"niece\", \"cousin\", \"always\",\n",
    "    \"mother\", \"wife\", \"grandma\", \"grandpa\", \"uncle\", \"aunt\",\n",
    "    \"courses\", \"psychiatrist\", \"medical\", \"%\", \"anesthesiologist\", \"professional\", \"surgeon\", \"specialists\",\n",
    "    \"intern\", \"worker\",\n",
    "}\n",
    "\n",
    "# 其他類別模糊詞彙\n",
    "# 基本是時間相關\n",
    "other_keywords = {\n",
    "    \"mm\", \"millimeter\", \"bed\", \"work\", \"psychiatrist\",\n",
    "    \"than\",\n",
    "    \"dinner\",\n",
    "    \"$\", \"little\",\n",
    "    \"page\",\n",
    "    \"pages\",\n",
    "    \"enough\",\n",
    "    \"old\",\n",
    "    \"always\",\n",
    "    \"regular\",\n",
    "    \"millimeter\",\n",
    "    \"senior\",\n",
    "    \"junior\",\n",
    "    \"through\",\n",
    "    \"step\",\n",
    "}\n",
    "\n",
    "# 常見模型生成字/全域刪除\n",
    "global_keywords = {\n",
    "    \"default\",\n",
    "    \"expression\",\n",
    "    \"none\",\n",
    "    \"specific\",\n",
    "    \"calculated\",\n",
    "    \"mentioned\",\n",
    "    \"explicitly\",\n",
    "    \"implied\",\n",
    "    \"null\",\n",
    "    \"there\",\n",
    "    \"think\",\n",
    "    \"name\",\n",
    "    \"awaiting\",\n",
    "    \"extracted\",\n",
    "    \"exactly\",\n",
    "    \"ongoing\",\n",
    "    \"number\",\n",
    "    \"united states\",\n",
    "    \"roche-cobas\",\n",
    "    \"tanzania\",\n",
    "    \"doctor\",\n",
    "    \"profession\",\n",
    "    \"foundation\",\n",
    "    \"%\",\n",
    "    \"milliliter\",\n",
    "    \"long time\",\n",
    "    \"few day\",\n",
    "    \"three days\",\n",
    "    \"couple\",\n",
    "}\n",
    "\n",
    "# ======== 讀取合法詞表（從 Validation_Dataset_Formal_entity.json 建立） ========\n",
    "\n",
    "valid_entities_by_category = {}\n",
    "with open(Validation_Dataset_Formal_entity, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        for cat, entities in item.items():\n",
    "            upper_cat = cat.upper()\n",
    "            valid_entities_by_category.setdefault(upper_cat, set())\n",
    "            for e in entities:\n",
    "                valid_entities_by_category[upper_cat].add(e.lower())\n",
    "\n",
    "# ======== 開始清理流程 ========\n",
    "invalid_entries = []\n",
    "\n",
    "with open(submission_task2_answer_rule2, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_cleaned, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or \"\\t\" not in line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "\n",
    "        fid, category, entity = parts\n",
    "        category = category.strip().upper()\n",
    "        entity = entity.strip()\n",
    "\n",
    "        # === 類別是否合法 ===\n",
    "        if category not in valid_categories:\n",
    "            invalid_entries.append((fid, category, entity))\n",
    "            continue\n",
    "\n",
    "        # === 是否為不合格的代名詞 ===\n",
    "        if category in invalid_categories and entity.lower() in invalid_pronouns:\n",
    "            invalid_entries.append((fid, category, entity))\n",
    "            continue\n",
    "\n",
    "        # === 防止日期型態誤被標為 ID 或 MRN ===\n",
    "        if category in {\"MEDICAL_RECORD_NUMBER\", \"ID_NUMBER\"}:\n",
    "            if re.search(r\"\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b\", entity):\n",
    "                invalid_entries.append((fid, category, entity))\n",
    "                continue\n",
    "            if re.match(r\".*\\b\\d{1,2}(st|nd|rd|th)?\\b[,/\\s]*\\d{4}\\b\", entity):\n",
    "                invalid_entries.append((fid, category, entity))\n",
    "                continue\n",
    "\n",
    "        # === 將 entity 拆詞（切分為 tokens）===\n",
    "        tokens = re.findall(r\"[a-zA-Z]+|\\d+|\\$\", entity.lower())\n",
    "\n",
    "        # === 如果是人物類別但仍出現 Dr. 前綴，代表標錯類，直接移除 ===\n",
    "        if category in [\"PATIENT\", \"PERSONALNAME\", \"FAMILYNAME\", \"PROFESSION\", \"AGE\"]:\n",
    "            if re.match(r\"^dr\\.?\\s+\", entity.lower()):\n",
    "                invalid_entries.append((fid, category, entity))\n",
    "                continue\n",
    "\n",
    "        # === 人物類別使用人物模糊詞（如 mom, dad）===\n",
    "        if category in [\"PATIENT\", \"DOCTOR\", \"PERSONALNAME\", \"FAMILYNAME\", \"PROFESSION\", \"AGE\"]:\n",
    "            if entity.lower() not in valid_entities_by_category.get(category, set()):\n",
    "                if any(t in person_keywords or t.rstrip('s') in person_keywords for t in tokens):\n",
    "                    invalid_entries.append((fid, category, entity))\n",
    "                    continue\n",
    "\n",
    "        # === 非核心類別使用其他模糊詞===\n",
    "        if category in invalid_categories or category not in [\n",
    "            \"PATIENT\", \"DOCTOR\", \"PERSONALNAME\", \"FAMILYNAME\",\n",
    "            'PROFESSION', 'ROOM', 'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION',\n",
    "            'STREET', 'CITY', 'DISTRICT', 'COUNTY', 'STATE', 'COUNTRY', 'LOCATION-OTHER',\n",
    "            \"MEDICAL_RECORD_NUMBER\", \"ID_NUMBER\"]:\n",
    "            if any(t in other_keywords for t in tokens):\n",
    "                invalid_entries.append((fid, category, entity))\n",
    "                continue\n",
    "\n",
    "        # === 全域詞排除 ===\n",
    "        if any(gk in entity.lower() for gk in global_keywords):\n",
    "            invalid_entries.append((fid, category, entity))\n",
    "            continue\n",
    "\n",
    "        # === 符合所有條件才寫入 ===\n",
    "        fout.write(f\"{fid}\\t{category}\\t{entity}\\n\")\n",
    "\n",
    "# ======== 顯示結果 ========\n",
    "print(f\"✅ 清理完成，合法結果寫入：{submission_task2_answer_cleaned}\")\n",
    "print(f\"被移除的不合法條目數：{len(invalid_entries)}\")\n",
    "if invalid_entries:\n",
    "    print(\"範例錯誤條目:\")\n",
    "    for x in invalid_entries:\n",
    "        print(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 如果precision>>recall，就加上5倍的量，提高recall，如果沒有就1倍，維持原樣"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "repeat_n = 1\n",
    "\n",
    "with open(submission_task2_answer_cleaned, encoding=\"utf-8\") as fin:\n",
    "    lines = [line.strip() for line in fin if line.strip()]  # 保留原始順序與內容\n",
    "\n",
    "with open(submission_task2_answer_duplicated, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for _ in range(repeat_n):\n",
    "        for line in lines:\n",
    "            fout.write(line + \"\\n\")\n",
    "print(f\"已乘 {repeat_n} 倍數到{submission_task2_answer_duplicated}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加上時間戳"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import unicodedata\n",
    "import json\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from num2words import num2words\n",
    "\n",
    "# ====== 工具函式 ======\n",
    "def normalize(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text).lower()\n",
    "    text = text.replace(\"’s\", \"s\").replace(\"'s\", \"s\").replace(\"‘\", \"\")\n",
    "    return re.sub(r\"[“”\\\".,:;!?]\", \"\", text).strip()\n",
    "\n",
    "def clean_entity(entity, category):\n",
    "    entity = entity.strip()\n",
    "    entity = re.sub(r\"^patient[,\\s]*\", \"\", entity, flags=re.I)\n",
    "    entity = re.sub(r\"^doctor[,\\s]*\", \"\", entity, flags=re.I)\n",
    "    entity = re.sub(r\"[.,\\s]*(with|received)$\", \"\", entity, flags=re.I)\n",
    "    return entity.strip()\n",
    "\n",
    "def is_similar(a, b, threshold=0.85):\n",
    "    return SequenceMatcher(None, a, b).ratio() >= threshold\n",
    "\n",
    "def is_digit_entity(entity):\n",
    "    return bool(re.match(r'^[\\dA-Z-]+$', entity))\n",
    "\n",
    "def convert_number_to_words(number):\n",
    "    if number.isdigit():\n",
    "        return num2words(int(number)).split()\n",
    "    return [c for c in number if c.isalnum()]\n",
    "\n",
    "def match_number_sequence(words, entity_words, max_gap=5, max_total_duration=6.0):\n",
    "    idx = 0\n",
    "    matched = []\n",
    "    for w in words:\n",
    "        word_norm = normalize(w['word'])\n",
    "        if idx < len(entity_words) and (word_norm == entity_words[idx] or entity_words[idx] in word_norm):\n",
    "            matched.append(w)\n",
    "            idx += 1\n",
    "        if idx == len(entity_words):\n",
    "            break\n",
    "    if idx == len(entity_words):\n",
    "        if matched[-1]['end'] - matched[0]['start'] <= max_total_duration:\n",
    "            for i in range(1, len(matched)):\n",
    "                if matched[i]['start'] - matched[i-1]['end'] > max_gap:\n",
    "                    return None\n",
    "            return matched\n",
    "    return None\n",
    "\n",
    "# ✅ 改良後：同名函數 overlaps，包含 ±0.05 秒 & >20% 時間重疊\n",
    "def overlaps(start, end, span_list, tolerance=0.05, min_overlap_ratio=0.2):\n",
    "    for s, e in span_list:\n",
    "        if abs(start - s) < tolerance and abs(end - e) < tolerance:\n",
    "            return True\n",
    "        inter_start = max(start, s)\n",
    "        inter_end = min(end, e)\n",
    "        intersection = max(0.0, inter_end - inter_start)\n",
    "        if intersection > 0:\n",
    "            duration = end - start\n",
    "            if duration > 0 and (intersection / duration) >= min_overlap_ratio:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# ====== 載入資料 ======\n",
    "with open(task1_answer_timestamps, encoding=\"utf-8\") as f:\n",
    "    timestamp_map = {json.loads(line)[\"filename\"]: json.loads(line)[\"words\"] for line in f}\n",
    "\n",
    "sentence_map = {}\n",
    "with open(submission_task1_answer, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if \"\\t\" in line:\n",
    "            fid, sentence = line.strip().split(\"\\t\", 1)\n",
    "            sentence_map[fid] = sentence\n",
    "\n",
    "not_found = []\n",
    "used_spans = {}\n",
    "\n",
    "# ====== 對齊流程 ======\n",
    "with open(submission_task2_answer_duplicated, encoding=\"utf-8\") as fin, \\\n",
    "     open(submission_task2_answer_alignment, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    total = success = fallback = 0\n",
    "\n",
    "    for line in fin:\n",
    "        total += 1\n",
    "        fid, category, entity = line.strip().split(\"\\t\")\n",
    "        sentence = sentence_map.get(fid, \"\")\n",
    "        if not sentence:\n",
    "            not_found.append((fid, category, entity, \"No sentence\"))\n",
    "            continue\n",
    "\n",
    "        words = timestamp_map.get(fid, [])\n",
    "        if not words:\n",
    "            not_found.append((fid, category, entity, \"No timestamp\"))\n",
    "            continue\n",
    "\n",
    "        entity_clean = clean_entity(entity, category)\n",
    "        entity_norm = normalize(entity_clean)\n",
    "\n",
    "        if fid not in used_spans:\n",
    "            used_spans[fid] = []\n",
    "\n",
    "        matched = False\n",
    "        was_overlapped = False\n",
    "\n",
    "        # === 精準比對 ===\n",
    "        for window in range(1, 6):\n",
    "            for i in range(len(words) - window + 1):\n",
    "                segment = words[i:i + window]\n",
    "                segment_text = \" \".join(w[\"word\"] for w in segment)\n",
    "                if normalize(segment_text) == entity_norm:\n",
    "                    start = float(segment[0][\"start\"])\n",
    "                    end = float(segment[-1][\"end\"])\n",
    "                    if overlaps(start, end, used_spans[fid]):\n",
    "                        was_overlapped = True\n",
    "                        continue\n",
    "                    fout.write(f\"{fid}\\t{category}\\t{start:.3f}\\t{end:.3f}\\t{entity}\\n\")\n",
    "                    used_spans[fid].append((start, end))\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "\n",
    "        # === 模糊單字比對 ===\n",
    "        if not matched:\n",
    "            for w in words:\n",
    "                word_norm = normalize(w[\"word\"])\n",
    "                if (word_norm.startswith(entity_norm) or entity_norm in word_norm or is_similar(word_norm, entity_norm)):\n",
    "                    start = float(w[\"start\"])\n",
    "                    end = float(w[\"end\"])\n",
    "                    if overlaps(start, end, used_spans[fid]):\n",
    "                        was_overlapped = True\n",
    "                        continue\n",
    "                    fout.write(f\"{fid}\\t{category}\\t{start:.3f}\\t{end:.3f}\\t{entity}\\n\")\n",
    "                    used_spans[fid].append((start, end))\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "        # === 數字實體比對 ===\n",
    "        if not matched and is_digit_entity(entity):\n",
    "            entity_words = convert_number_to_words(entity.replace('-', ''))\n",
    "            matched_seq = match_number_sequence(words, entity_words)\n",
    "            if matched_seq:\n",
    "                start = float(matched_seq[0][\"start\"])\n",
    "                end = float(matched_seq[-1][\"end\"])\n",
    "                if overlaps(start, end, used_spans[fid]):\n",
    "                    was_overlapped = True\n",
    "                else:\n",
    "                    fout.write(f\"{fid}\\t{category}\\t{start:.3f}\\t{end:.3f}\\t{entity}\\n\")\n",
    "                    used_spans[fid].append((start, end))\n",
    "                    matched = True\n",
    "\n",
    "        # fallback 比對應該在這個 if not matched: 裡面\n",
    "        if not matched:\n",
    "            for window in range(1, 8):  # 可調整最大長度\n",
    "                for i in range(len(words) - window + 1):\n",
    "                    segment = words[i:i + window]\n",
    "                    segment_text = \" \".join(w[\"word\"] for w in segment)\n",
    "                    sim_ratio = SequenceMatcher(None, normalize(segment_text), entity_norm).ratio()\n",
    "\n",
    "                    if sim_ratio >= 0.75:\n",
    "                        start = float(segment[0][\"start\"])\n",
    "                        end = float(segment[-1][\"end\"])\n",
    "\n",
    "                        if overlaps(start, end, used_spans.get(fid, [])):\n",
    "                            continue\n",
    "\n",
    "                        fout.write(f\"{fid}\\t{category}\\t{start:.3f}\\t{end:.3f}\\t{entity}\\n\")\n",
    "                        used_spans.setdefault(fid, []).append((start, end))\n",
    "                        fallback += 1\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "\n",
    "\n",
    "        if not matched:\n",
    "            reason = \"時間戳重疊\" if was_overlapped else \"Cannot align\"\n",
    "            not_found.append((fid, category, entity, reason))\n",
    "        else:\n",
    "            success += 1\n",
    "\n",
    "# ====== 結果輸出 ======\n",
    "print(f\"\\n✅ 對齊完成！總筆數: {total}，成功: {success}，fallback: {fallback}，失敗: {len(not_found)}\")\n",
    "print(\"📌 無法對齊或被排除的項目:\")\n",
    "for item in not_found:\n",
    "    print(item)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 硬抓中文時間戳"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "Validation_Dataset_Formal_entity= base_path / \"Validation_Dataset_Formal_entity.json\"\n",
    "\n",
    "# === 定義硬抓字串對應的類別 ===\n",
    "hardcoded_patterns = {\n",
    "    \"SET\": [r\"[一二三四五六七八九十]次\"],\n",
    "    \"PROFESSION\": [],\n",
    "    \"COUNTRY\": [],\n",
    "    \"DURATION\": [r\"[幾几一二两三四五六七八九十]+(?:年半|天)\"],\n",
    "    \"ROOM\": [r\"[一二两三四五六七八九十]+(?:台|床)\"]\n",
    "}\n",
    "\n",
    "# === 保留原有正則 pattern ===\n",
    "partial_familyname_patterns = [r\"[赵钱孙李吴郑王冯陈褚卫蒋沉韩杨朱秦尤许何吕施张劉孔曹严华金魏陶姜](太太|先生)\"]\n",
    "partial_personalname_patterns = []\n",
    "\n",
    "# === 載入 JSON 路徑變數（Validation_Dataset_Formal_entity） ===\n",
    "with open(Validation_Dataset_Formal_entity, \"r\", encoding=\"utf-8\") as f:\n",
    "    Validation_Dataset_Formal_entity = json.load(f)\n",
    "\n",
    "def extract_list_by_key(entity_list, key):\n",
    "    for item in entity_list:\n",
    "        if key in item:\n",
    "            return item[key]\n",
    "    return []\n",
    "\n",
    "full_familyname_patterns = [re.escape(name) for name in extract_list_by_key(Validation_Dataset_Formal_entity, \"FAMILYNAME\")]\n",
    "full_personalname_patterns = [re.escape(name) for name in extract_list_by_key(Validation_Dataset_Formal_entity, \"PERSONALNAME\")]\n",
    "hardcoded_patterns[\"PROFESSION\"] = [re.escape(name) for name in extract_list_by_key(Validation_Dataset_Formal_entity, \"PROFESSION\")]\n",
    "hardcoded_patterns[\"COUNTRY\"] = [re.escape(name) for name in extract_list_by_key(Validation_Dataset_Formal_entity, \"COUNTRY\")]\n",
    "\n",
    "special_categories = {\"FAMILYNAME\", \"PERSONALNAME\", \"COUNTRY\", \"PROFESSION\", \"SET\", \"DURATION\", \"ROOM\"}\n",
    "surname_set = set(\"赵钱孙李吴郑王冯陈褚卫蒋沉韩杨朱秦尤许何吕施张劉孔曹严华金魏陶姜\")\n",
    "\n",
    "# === 暫存原始檔內容 ===\n",
    "with open(submission_task2_answer_alignment, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# === 建立原始標註表，依 fid 分類 ===\n",
    "original_by_fid = {}\n",
    "for line in original_lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 5:\n",
    "        continue\n",
    "    fid, tag = parts[0], parts[1]\n",
    "    original_by_fid.setdefault(fid, []).append(line)\n",
    "\n",
    "# === 開始處理 ===\n",
    "new_outputs_by_fid = {}\n",
    "processed_fids = set()\n",
    "\n",
    "with open(task1_answer_timestamps_ZH, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            fid = data[\"filename\"]\n",
    "            fid_int = int(fid)\n",
    "            words = data.get(\"words\", [])\n",
    "\n",
    "            word_list = [w[\"word\"] for w in words]\n",
    "            word_spans = [(w[\"start\"], w[\"end\"]) for w in words]\n",
    "            full_text = \"\".join(word_list)\n",
    "\n",
    "            output_lines = []\n",
    "            skip_positions = set()\n",
    "            profession_spans = []\n",
    "\n",
    "            def is_within_existing_span(start, end, spans):\n",
    "                for s, e in spans:\n",
    "                    if start >= s and end <= e:\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "            def write_tag(tag, match_text, start_char, end_char):\n",
    "                char_index = 0\n",
    "                start_index = end_index = None\n",
    "                for idx, word in enumerate(word_list):\n",
    "                    word_len = len(word)\n",
    "                    next_index = char_index + word_len\n",
    "                    if start_index is None and start_char < next_index:\n",
    "                        start_index = idx\n",
    "\n",
    "                    if start_char < next_index and end_char <= next_index:\n",
    "                        end_index = idx\n",
    "                        break\n",
    "\n",
    "                    char_index = next_index\n",
    "                if start_index is not None and end_index is not None:\n",
    "                    start_time = word_spans[start_index][0]\n",
    "                    end_time = word_spans[end_index][1]\n",
    "                    line = f\"{fid}\\t{tag}\\t{start_time:.3f}\\t{end_time:.3f}\\t{match_text}\"\n",
    "                    output_lines.append(line)\n",
    "                    if tag in special_categories:\n",
    "                        print(f\"[{fid}] {tag}：{match_text}（{start_time:.3f}s ~ {end_time:.3f}s）\")\n",
    "\n",
    "            if fid_int < 80000:\n",
    "                output_lines.extend(original_by_fid.get(fid, []))\n",
    "            else:\n",
    "                processed_fids.add(fid)\n",
    "                original_tags = original_by_fid.get(fid, [])\n",
    "                retained_lines = [l for l in original_tags if l.split(\"\\t\")[1] not in special_categories]\n",
    "                output_lines.extend(retained_lines)\n",
    "\n",
    "                for category, patterns in hardcoded_patterns.items():\n",
    "                    if category not in special_categories:\n",
    "                        continue\n",
    "                    for pattern in patterns:\n",
    "                        for match in re.finditer(pattern, full_text):\n",
    "                            match_text = match.group()\n",
    "                            start = match.start()\n",
    "                            end = match.end()\n",
    "\n",
    "                            if category == \"PROFESSION\":\n",
    "                                name_preceded_professions = {\n",
    "                                    \"法官\", \"律師\", \"老師\", \"導師\", \"法師\", \"牧師\", \"講師\",\n",
    "                                    \"律师\", \"老师\", \"导师\", \"法师\", \"牧师\", \"讲师\"\n",
    "                                }\n",
    "\n",
    "                                if match_text in name_preceded_professions:\n",
    "                                    char_pos = 0\n",
    "                                    for i in range(len(word_list)):\n",
    "                                        if char_pos == start:\n",
    "                                            if i >= 2:\n",
    "                                                name_candidate = word_list[i-2] + word_list[i-1]\n",
    "                                                if word_list[i-1] in surname_set:\n",
    "                                                    name_start = sum(len(word_list[j]) for j in range(i-1))\n",
    "                                                    name_end = sum(len(word_list[j]) for j in range(i))\n",
    "                                                    write_tag(\"PERSONALNAME\", word_list[i-1], name_start, name_end)\n",
    "                                                else:\n",
    "                                                    name_start = sum(len(word_list[j]) for j in range(i-2))\n",
    "                                                    name_end = sum(len(word_list[j]) for j in range(i))\n",
    "                                                    write_tag(\"PERSONALNAME\", name_candidate, name_start, name_end)\n",
    "\n",
    "                                                skip_positions.add((start, end))\n",
    "                                                profession_spans.append((start, end))\n",
    "                                                break\n",
    "                                        char_pos += len(word_list[i])\n",
    "\n",
    "                                if is_within_existing_span(start, end, profession_spans):\n",
    "                                    continue\n",
    "\n",
    "                                if start > 0:\n",
    "                                    prev_char = full_text[start - 1]\n",
    "                                    if prev_char in surname_set:\n",
    "                                        combined = prev_char + match_text\n",
    "                                        skip_positions.add((start - 1, end))\n",
    "                                        profession_spans.append((start - 1, end))\n",
    "                                        write_tag(\"PERSONALNAME\", combined, start - 1, end)\n",
    "                                        continue\n",
    "\n",
    "                                    elif match_text in [\n",
    "                                        \"作家\", \"校長\", \"記者\", \"委員\", \"主管\", \"顧問\", \"立委\", \"司機\", \"議員\",\n",
    "                                        \"作家\", \"校长\", \"记者\", \"委员\", \"主管\", \"顾问\", \"立委\", \"司机\", \"议员\"\n",
    "                                    ]:\n",
    "                                        prior_text = full_text[max(0, start - 1):start]\n",
    "                                        if not any(c in surname_set for c in prior_text):\n",
    "                                            continue\n",
    "\n",
    "                                profession_spans.append((start, end))\n",
    "                                write_tag(\"PROFESSION\", match_text, start, end)\n",
    "                                continue\n",
    "\n",
    "                            # 其他類別照常處理\n",
    "                            if (start, end) in skip_positions:\n",
    "                                continue\n",
    "                            write_tag(category, match_text, start, end)\n",
    "\n",
    "                for pattern in partial_familyname_patterns:\n",
    "                    for match in re.finditer(pattern, full_text):\n",
    "                        write_tag(\"FAMILYNAME\", match.group()[0], match.start(), match.start() + 1)\n",
    "                for pattern in full_familyname_patterns:\n",
    "                    for match in re.finditer(pattern, full_text):\n",
    "                        write_tag(\"FAMILYNAME\", match.group(), match.start(), match.end())\n",
    "\n",
    "                for pattern in partial_personalname_patterns:\n",
    "                    for match in re.finditer(pattern, full_text):\n",
    "                        write_tag(\"PERSONALNAME\", match.group()[0], match.start(), match.start() + 1)\n",
    "                for pattern in full_personalname_patterns:\n",
    "                    for match in re.finditer(pattern, full_text):\n",
    "                        write_tag(\"PERSONALNAME\", match.group(), match.start(), match.end())\n",
    "\n",
    "\n",
    "            new_outputs_by_fid[fid] = output_lines\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 處理 {data.get('filename', '<UNKNOWN>')} 失敗：{e}\")\n",
    "            continue\n",
    "\n",
    "# === 將未出現在 timestamp 的 <80000 的 fid 也補回來 ===\n",
    "for fid, lines in original_by_fid.items():\n",
    "    if int(fid) < 80000 and fid not in new_outputs_by_fid:\n",
    "        new_outputs_by_fid[fid] = lines\n",
    "\n",
    "with open(submission_task2_answer_alignment, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for lines in new_outputs_by_fid.values():\n",
    "        for line in lines:\n",
    "            fout.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\n✅ 寫入：{submission_task2_answer_alignment}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 去重"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# === 讀取 timestamp 對應的 word list ===\n",
    "timestamp_map = {}\n",
    "with open(task1_answer_timestamps, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        timestamp_map[data[\"filename\"]] = data[\"words\"]\n",
    "\n",
    "# 統計\n",
    "fallback_added = 0\n",
    "failed = 0\n",
    "output_entries = []  # 使用 dict 儲存每筆資料，便於時間比對\n",
    "\n",
    "# === 計算時間重疊比例 ===\n",
    "def time_overlap_ratio(a_start, a_end, b_start, b_end):\n",
    "    overlap = max(0, min(a_end, b_end) - max(a_start, b_start))\n",
    "    duration = min(a_end - a_start, b_end - b_start)\n",
    "    return overlap / duration if duration > 0 else 0\n",
    "\n",
    "# === 嘗試加入不重疊的項目 ===\n",
    "def add_if_not_overlap(new_entry):\n",
    "    for existing in output_entries:\n",
    "        if (new_entry[\"fid\"] == existing[\"fid\"] and\n",
    "            new_entry[\"category\"] == existing[\"category\"] and\n",
    "            new_entry[\"entity\"] == existing[\"entity\"]):\n",
    "\n",
    "            ratio = time_overlap_ratio(\n",
    "                new_entry[\"start\"], new_entry[\"end\"],\n",
    "                existing[\"start\"], existing[\"end\"]\n",
    "            )\n",
    "            if ratio > 0.4:\n",
    "                return False  # 重疊比例過高，不加\n",
    "\n",
    "    output_entries.append(new_entry)\n",
    "    return True\n",
    "\n",
    "# === 開始處理 alignment ===\n",
    "with open(submission_task2_answer_alignment, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line or line.upper() == \"PHI:NULL\":\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 5:\n",
    "            print(f\"[ERROR] 格式錯誤：{line}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        fid, category, start, end, entity = parts\n",
    "\n",
    "        # 已有合法時間戳 → 驗證是否為 float\n",
    "        try:\n",
    "            start_f = float(start)\n",
    "            end_f = float(end)\n",
    "            entry = {\n",
    "                \"fid\": fid,\n",
    "                \"category\": category,\n",
    "                \"start\": start_f,\n",
    "                \"end\": end_f,\n",
    "                \"entity\": entity,\n",
    "                \"raw_line\": line\n",
    "            }\n",
    "            add_if_not_overlap(entry)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            pass  # fallback 處理\n",
    "\n",
    "        # fallback：從 timestamp map 嘗試比對\n",
    "        words = timestamp_map.get(fid, [])\n",
    "        if not words:\n",
    "            print(f\"[SKIP] 無對應 timestamp：{fid}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        entity_tokens = list(entity)  # 中文每字一 token\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            idx = 0\n",
    "            matched_words = []\n",
    "\n",
    "            for j in range(i, len(words)):\n",
    "                word = words[j][\"word\"]\n",
    "                if idx < len(entity_tokens) and word == entity_tokens[idx]:\n",
    "                    matched_words.append(words[j])\n",
    "                    idx += 1\n",
    "                    if idx == len(entity_tokens):\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if idx == len(entity_tokens):\n",
    "                new_start = float(matched_words[0][\"start\"])\n",
    "                new_end = float(matched_words[-1][\"end\"])\n",
    "                raw_line = f\"{fid}\\t{category}\\t{new_start:.3f}\\t{new_end:.3f}\\t{entity}\"\n",
    "                entry = {\n",
    "                    \"fid\": fid,\n",
    "                    \"category\": category,\n",
    "                    \"start\": new_start,\n",
    "                    \"end\": new_end,\n",
    "                    \"entity\": entity,\n",
    "                    \"raw_line\": raw_line\n",
    "                }\n",
    "                if add_if_not_overlap(entry):\n",
    "                    fallback_added += 1\n",
    "                    print(f\"[FALLBACK] {fid} {entity} \\u2794 {new_start:.3f}s ~ {new_end:.3f}s\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"[MISS] 無法補時間：{fid}\\t{entity}\")\n",
    "            failed += 1\n",
    "\n",
    "# === 寫入結果（去重後） ===\n",
    "with open(submission_task2_answer_sort, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(\"\\n\".join(sorted(entry[\"raw_line\"] for entry in output_entries)) + \"\\n\")\n",
    "\n",
    "# === 統計輸出 ===\n",
    "print(f\"\\n✅ 完成！最終輸出筆數：{len(output_entries)}\")\n",
    "print(f\"🔁 fallback 成功補時間：{fallback_added}\")\n",
    "print(f\"❌ 無法處理（格式錯或無 timestamp）：{failed}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 排序+計算數量"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# ======== 定義類別順序 ========\n",
    "category_order = [\n",
    "    'PATIENT', 'DOCTOR', 'USERNAME', 'PERSONALNAME', 'FAMILYNAME', 'PROFESSION',\n",
    "    'ROOM', 'DEPARTMENT', 'HOSPITAL', 'ORGANIZATION', 'STREET', 'CITY', 'STATE',\n",
    "    'COUNTRY', 'COUNTY', 'ZIP', 'LOCATION-OTHER', 'DISTRICT', 'AGE', 'DATE',\n",
    "    'TIME', 'DURATION', 'SET', 'PHONE', 'FAX', 'EMAIL', 'URL', 'IPADDRESS',\n",
    "    'OTHER', 'SOCIAL_SECURITY_NUMBER', 'MEDICAL_RECORD_NUMBER', 'HEALTH_PLAN_NUMBER',\n",
    "    'ACCOUNT_NUMBER', 'LICENSE_NUMBER', 'VEHICLE_ID', 'DEVICE_ID', 'BIOMETRIC_ID',\n",
    "    'ID_NUMBER'\n",
    "]\n",
    "\n",
    "# ======== 統計各類別出現次數 ========\n",
    "category_counter = Counter()\n",
    "\n",
    "with open(submission_task2_answer_sort, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 5:  # 格式: fid, category, start, end, entity\n",
    "            _, category, _, _, _ = parts\n",
    "            category = category.strip().upper()\n",
    "            category_counter[category] += 1\n",
    "\n",
    "# ======== 顯示原始統計（照最初的格式）========\n",
    "for cat in category_order:\n",
    "    print(f\"{cat}: {category_counter.get(cat, 0)}\")\n",
    "\n",
    "# ======== 排序（依 fid → start → end）========\n",
    "entries = []\n",
    "\n",
    "with open(submission_task2_answer_sort, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 5:\n",
    "            fid, category, start, end, entity = parts\n",
    "            try:\n",
    "                fid_num = int(fid)\n",
    "                start_num = float(start)\n",
    "                end_num = float(end)\n",
    "                entries.append((fid_num, start_num, end_num, fid, category, entity))\n",
    "            except ValueError:\n",
    "                print(f\"[SKIP] 非法 fid/start/end 無法轉為整數：{line}\")\n",
    "                continue\n",
    "\n",
    "# ======== 排序後寫入最終檔案========\n",
    "entries.sort(key=lambda x: (x[0], x[1], x[2]))\n",
    "\n",
    "with open(submission_task2_answer_finally, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for _, start_num, end_num, fid, category, entity in entries:\n",
    "        fout.write(f\"{fid}\\t{category}\\t{start_num}\\t{end_num}\\t{entity}\\n\")\n",
    "\n",
    "print(f\"\\nTOTAL: {sum(category_counter.values())}\")\n",
    "print(f\"\\n\\n{submission_task2_answer_finally}排序已完成\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
